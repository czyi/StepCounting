cross validation  0
Initialized
Loss at step 0: 1.001784, train accuarcy : 0.349296
Loss at step 200: 0.165952, train accuarcy : 0.788732
Loss at step 400: 0.134667, train accuarcy : 0.842254
Loss at step 600: 0.123531, train accuarcy : 0.845070
Loss at step 800: 0.116326, train accuarcy : 0.867606
Loss at step 1000: 0.107630, train accuarcy : 0.918310
Loss at step 1200: 0.099792, train accuarcy : 0.887324
Loss at step 1400: 0.093469, train accuarcy : 0.830986
Loss at step 1600: 0.086585, train accuarcy : 0.938028
Loss at step 1800: 0.089843, train accuarcy : 0.940845
Loss at step 2000: 0.079369, train accuarcy : 0.904225
Loss at step 2200: 0.073753, train accuarcy : 0.932394
Loss at step 2400: 0.077801, train accuarcy : 0.935211
Loss at step 2600: 0.072208, train accuarcy : 0.929577
Loss at step 2800: 0.070046, train accuarcy : 0.938028
Loss at step 3000: 0.070779, train accuarcy : 0.918310
Loss at step 3200: 0.066193, train accuarcy : 0.923944
Loss at step 3400: 0.063207, train accuarcy : 0.912676
Loss at step 3600: 0.063215, train accuarcy : 0.932394
Loss at step 3800: 0.065374, train accuarcy : 0.957746
Loss at step 4000: 0.063806, train accuarcy : 0.954930
cross validation  1
Initialized
Loss at step 0: 0.611603, train accuarcy : 0.447887
Loss at step 200: 0.137574, train accuarcy : 0.859155
Loss at step 400: 0.121416, train accuarcy : 0.864789
Loss at step 600: 0.105484, train accuarcy : 0.887324
Loss at step 800: 0.096812, train accuarcy : 0.892958
Loss at step 1000: 0.082924, train accuarcy : 0.830986
Loss at step 1200: 0.079735, train accuarcy : 0.909859
Loss at step 1400: 0.080630, train accuarcy : 0.907042
Loss at step 1600: 0.075562, train accuarcy : 0.890141
Loss at step 1800: 0.069931, train accuarcy : 0.929577
Loss at step 2000: 0.070680, train accuarcy : 0.929577
Loss at step 2200: 0.073087, train accuarcy : 0.940845
Loss at step 2400: 0.066126, train accuarcy : 0.949296
Loss at step 2600: 0.066629, train accuarcy : 0.940845
Loss at step 2800: 0.066422, train accuarcy : 0.909859
Loss at step 3000: 0.069597, train accuarcy : 0.943662
Loss at step 3200: 0.062043, train accuarcy : 0.923944
Loss at step 3400: 0.064517, train accuarcy : 0.929577
Loss at step 3600: 0.062199, train accuarcy : 0.929577
Loss at step 3800: 0.058441, train accuarcy : 0.929577
Loss at step 4000: 0.056483, train accuarcy : 0.915493
cross validation  2
Initialized
Loss at step 0: 1.023193, train accuarcy : 0.380282
Loss at step 200: 0.169278, train accuarcy : 0.850704
Loss at step 400: 0.143012, train accuarcy : 0.873239
Loss at step 600: 0.128046, train accuarcy : 0.816901
Loss at step 800: 0.116650, train accuarcy : 0.887324
Loss at step 1000: 0.104934, train accuarcy : 0.870423
Loss at step 1200: 0.094286, train accuarcy : 0.918310
Loss at step 1400: 0.091163, train accuarcy : 0.901408
Loss at step 1600: 0.094712, train accuarcy : 0.895775
Loss at step 1800: 0.082779, train accuarcy : 0.926761
Loss at step 2000: 0.083801, train accuarcy : 0.884507
Loss at step 2200: 0.080276, train accuarcy : 0.926761
Loss at step 2400: 0.079474, train accuarcy : 0.912676
Loss at step 2600: 0.076648, train accuarcy : 0.853521
Loss at step 2800: 0.067755, train accuarcy : 0.935211
Loss at step 3000: 0.070725, train accuarcy : 0.935211
Loss at step 3200: 0.073340, train accuarcy : 0.929577
Loss at step 3400: 0.068102, train accuarcy : 0.938028
Loss at step 3600: 0.065455, train accuarcy : 0.946479
Loss at step 3800: 0.066293, train accuarcy : 0.940845
Loss at step 4000: 0.065414, train accuarcy : 0.943662
cross validation  3
Initialized
Loss at step 0: 0.528447, train accuarcy : 0.487324
Loss at step 200: 0.154347, train accuarcy : 0.805634
Loss at step 400: 0.126977, train accuarcy : 0.881690
Loss at step 600: 0.107785, train accuarcy : 0.892958
Loss at step 800: 0.092044, train accuarcy : 0.912676
Loss at step 1000: 0.099676, train accuarcy : 0.907042
Loss at step 1200: 0.092665, train accuarcy : 0.878873
Loss at step 1400: 0.081808, train accuarcy : 0.923944
Loss at step 1600: 0.078203, train accuarcy : 0.901408
Loss at step 1800: 0.076615, train accuarcy : 0.918310
Loss at step 2000: 0.076112, train accuarcy : 0.918310
Loss at step 2200: 0.075127, train accuarcy : 0.923944
Loss at step 2400: 0.070417, train accuarcy : 0.923944
Loss at step 2600: 0.069070, train accuarcy : 0.943662
Loss at step 2800: 0.068938, train accuarcy : 0.926761
Loss at step 3000: 0.066235, train accuarcy : 0.938028
Loss at step 3200: 0.066090, train accuarcy : 0.918310
Loss at step 3400: 0.063317, train accuarcy : 0.901408
Loss at step 3600: 0.064399, train accuarcy : 0.938028
Loss at step 3800: 0.061580, train accuarcy : 0.926761
Loss at step 4000: 0.059883, train accuarcy : 0.946479
cross validation  4
Initialized
Loss at step 0: 0.662552, train accuarcy : 0.430986
Loss at step 200: 0.144827, train accuarcy : 0.791549
Loss at step 400: 0.123203, train accuarcy : 0.876056
Loss at step 600: 0.110742, train accuarcy : 0.861972
Loss at step 800: 0.100806, train accuarcy : 0.873239
Loss at step 1000: 0.096877, train accuarcy : 0.884507
Loss at step 1200: 0.084186, train accuarcy : 0.907042
Loss at step 1400: 0.083066, train accuarcy : 0.935211
Loss at step 1600: 0.085775, train accuarcy : 0.898592
Loss at step 1800: 0.080533, train accuarcy : 0.895775
Loss at step 2000: 0.072279, train accuarcy : 0.912676
Loss at step 2200: 0.073373, train accuarcy : 0.946479
Loss at step 2400: 0.072394, train accuarcy : 0.949296
Loss at step 2600: 0.071732, train accuarcy : 0.929577
Loss at step 2800: 0.069879, train accuarcy : 0.907042
Loss at step 3000: 0.064683, train accuarcy : 0.932394
Loss at step 3200: 0.068327, train accuarcy : 0.923944
Loss at step 3400: 0.061952, train accuarcy : 0.952113
Loss at step 3600: 0.062192, train accuarcy : 0.923944
Loss at step 3800: 0.061978, train accuarcy : 0.938028
Loss at step 4000: 0.058630, train accuarcy : 0.960563
cross validation  5
Initialized
Loss at step 0: 1.507989, train accuarcy : 0.200000
Loss at step 200: 0.184914, train accuarcy : 0.754930
Loss at step 400: 0.159750, train accuarcy : 0.836620
Loss at step 600: 0.136233, train accuarcy : 0.856338
Loss at step 800: 0.119903, train accuarcy : 0.864789
Loss at step 1000: 0.114560, train accuarcy : 0.907042
Loss at step 1200: 0.110898, train accuarcy : 0.873239
Loss at step 1400: 0.099081, train accuarcy : 0.915493
Loss at step 1600: 0.095280, train accuarcy : 0.887324
Loss at step 1800: 0.086488, train accuarcy : 0.909859
Loss at step 2000: 0.087447, train accuarcy : 0.915493
Loss at step 2200: 0.082313, train accuarcy : 0.938028
Loss at step 2400: 0.080628, train accuarcy : 0.915493
Loss at step 2600: 0.078551, train accuarcy : 0.923944
Loss at step 2800: 0.075279, train accuarcy : 0.940845
Loss at step 3000: 0.076449, train accuarcy : 0.929577
Loss at step 3200: 0.072200, train accuarcy : 0.938028
Loss at step 3400: 0.068985, train accuarcy : 0.921127
Loss at step 3600: 0.065125, train accuarcy : 0.946479
Loss at step 3800: 0.069082, train accuarcy : 0.918310
Loss at step 4000: 0.067990, train accuarcy : 0.918310