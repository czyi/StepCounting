{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('ISO-8859-1')\n",
    "\n",
    "''' Load the training set '''\n",
    "pd.set_option('display.max_column', None)\n",
    "training_set = pd.read_csv('train.csv')\n",
    "print \"size of training_set = \" + str(np.shape(training_set))\n",
    "\n",
    "# we want to separate the txt messages into two groups: ham(1) and spam(0)\n",
    "ham_or_spam = training_set['label']\n",
    "training_set_target = []\n",
    "for i in range(len(ham_or_spam)):\n",
    "    if ham_or_spam[i] == \"ham\":\n",
    "        training_set_target.append(1)\n",
    "    else:\n",
    "        training_set_target.append(0)\n",
    "\n",
    "# store the array in y, because below we transform this one to a matrix\n",
    "y = training_set_target\n",
    "training_set_target = np.transpose(np.matrix(training_set_target))\n",
    "print \"size of training_set_target = \" + str(np.shape(training_set_target))\n",
    "\n",
    "training_set_content = training_set['sms'].as_matrix()\n",
    "print \"number of text messages = \" + str(np.shape(training_set_content))\n",
    "print training_set_content[:20]\n",
    "\n",
    "''' Load the stopwords '''\n",
    "stop_words = stopwords.words('english')\n",
    "print \"We use %d stopwords from nltk library, such as:\" % np.shape(stop_words)[0]\n",
    "print stop_words[:10]\n",
    "\n",
    "''' Tokenization '''\n",
    "def tokenization(text):\n",
    "    tokens=[]\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "training_set_corpus_tokenized = []\n",
    "for i in training_set_content:\n",
    "    training_set_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "''' Applying TF-IDF '''\n",
    "vectorizer = CountVectorizer(decode_error = 'ignore')\n",
    "transformer = TfidfTransformer(norm = 'l2', use_idf = True)\n",
    "tfidf_matrix = transformer.fit_transform(vectorizer.fit_transform(training_set_corpus_tokenized))\n",
    "\n",
    "print \"size of tf-idf matrix = \" + str(np.shape(tfidf_matrix))\n",
    "word = vectorizer.get_feature_names()\n",
    "\n",
    "X = tfidf_matrix.toarray()\n",
    "print \"size of X = \" + str(np.shape(X))\n",
    "\n",
    "''' Define function for logistic regression '''\n",
    "def sigmoid(z):\n",
    "    sigma = 1 / (1 + np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "def loss_function(X, y, w, lmd):\n",
    "    # X is the training set, 3000 x 3000, w is the weight vector, 3000 x 1\n",
    "    # y is the target, 3000 x 1, lmd is the hyper parameter for regularization\n",
    "    m = np.shape(y)[0]\n",
    "    y = np.transpose(y)\n",
    "    hypo = sigmoid(np.dot(X, w))\n",
    "    # avoid the case of log(0)\n",
    "    for i in range(len(hypo)):\n",
    "        if hypo[i][0] > 0.99999:\n",
    "            hypo[i][0] = 0.99999\n",
    "        elif hypo[i][0] < 0.00001:\n",
    "            hypo[i][0] = 0.00001\n",
    "    loss = (1.0 / m) * (-np.dot(y, np.log(hypo)) - np.dot((1 - y), np.log(1 - hypo))) + lmd * np.dot(np.transpose(w), w)\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X, y, w, eta, lmd):\n",
    "    m = np.shape(y)[0]\n",
    "    hypo = sigmoid(np.dot(X, w))\n",
    "    loss_history = []\n",
    "    prev_loss = 9999.0\n",
    "    for i in range(1, 1001):\n",
    "        w = w - (1.0 / m) * eta * np.power(i, -0.9) * np.dot(np.transpose(X), hypo - y) + lmd * w\n",
    "        crt_loss = loss_function(X, y, w, lmd)[0][0]\n",
    "        # break when loss starts to grow\n",
    "        if crt_loss > prev_loss:\n",
    "            break\n",
    "        loss_history.append(crt_loss)\n",
    "        prev_loss = crt_loss\n",
    "    print loss_history\n",
    "    return w\n",
    "\n",
    "def predict(X, y, w):\n",
    "    predict_hypo = sigmoid(np.dot(X, w))\n",
    "    p = predict_hypo\n",
    "    for i in range(np.shape(y)[0]):\n",
    "        if predict_hypo[i][0] >= 0.5:\n",
    "            p[i][0] = 1\n",
    "        else:\n",
    "            p[i][0] = 0\n",
    "    return p\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "def compute_accuracy(X, y, w):\n",
    "    result = predict(X, y, w)\n",
    "    wrong_answer = 0.0\n",
    "    for i in range(np.shape(y)[0]):\n",
    "        if result[i][0] != y[i][0]:\n",
    "            wrong_answer = wrong_answer + 1\n",
    "    accuracy = 1 - wrong_answer / np.shape(y)[0]\n",
    "    return accuracy\n",
    "#     print \"accuracy_my_model = \", accuracy_score(result, training_set_target);\n",
    "#     print \"precision_my_model = \", precision_score(result, training_set_target);\n",
    "#     print \"recall_my_model = \", recall_score(result, training_set_target);    \n",
    "\n",
    "# initialize w to be a matrix of dimension 3000 x 1 with all elements to be 0\n",
    "w = np.transpose(np.matrix(np.zeros(np.shape(X)[1])))\n",
    "w = gradient_descent(X, training_set_target, w, 1, 0)\n",
    "print w\n",
    "\n",
    "accuracy = compute_accuracy(X, training_set_target, w)\n",
    "print \"accuracy_my_model = \" + str(accuracy)\n",
    "\n",
    "''' Load the test data and calculate the accuracy '''\n",
    "test_set = pd.read_csv('test.csv')\n",
    "print \"size of test_set = \" + str(np.shape(test_set))\n",
    "\n",
    "ham_or_spam = test_set['label']\n",
    "test_set_target = []\n",
    "for i in range(len(ham_or_spam)):\n",
    "    if ham_or_spam[i] == \"ham\":\n",
    "        test_set_target.append(1)\n",
    "    else:\n",
    "        test_set_target.append(0)\n",
    "y_test = test_set_target\n",
    "test_set_target = np.transpose(np.matrix(test_set_target))\n",
    "print \"size of test_set_target = \" + str(np.shape(test_set_target))\n",
    "\n",
    "test_set_content = test_set['sms'].as_matrix()\n",
    "test_set_corpus_tokenized = []\n",
    "for i in test_set_content:\n",
    "    test_set_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "print \"size of test_set_corpus_tokenized = \" + str(np.shape(test_set_corpus_tokenized))\n",
    "tfidf_test = transformer.transform(vectorizer.transform(test_set_corpus_tokenized))\n",
    "print \"size of tfidf_test = \" + str(np.shape(tfidf_test))\n",
    "\n",
    "X_test = tfidf_test.toarray()\n",
    "print \"size of X_test = \" + str(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 100,\n",
       " [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_history=[]\n",
    "from sklearn.cross_validation import KFold \n",
    "# return the lmd that maximize the accuracy\n",
    "def cross_validation(X, w, y, eta):\n",
    "    # divide the dataset into 10 folds\n",
    "    #kf = KFold(len(training_set_target), n_folds = 10)\n",
    "    training_cost_avg = 0\n",
    "    test_cost_avg = 0\n",
    "    #accuracy_history = []\n",
    "    #for train_index, test_index in kf:\n",
    "    #for i in range(0,10):\n",
    "    i=2\n",
    "    X_train, X_test = np.vstack((X[0:i*300],X[(i+1)*300:3000])), X[i*300:(i+1)*300]\n",
    "    y_train, y_test = np.vstack((y[0:i*300],y[(i+1)*300:3000])), y[i*300:(i+1)*300]\n",
    "    lmd = np.linspace(0, 100, num=100)\n",
    "    for i in range(len(lmd)):\n",
    "        w = np.transpose(np.matrix(np.zeros(np.shape(X_train)[1])))\n",
    "        w = gradient_descent(X_train, y_train, w, eta, lmd[i])\n",
    "        accuracy = compute_accuracy(X_test, y_test, w)\n",
    "        accuracy_history.append(accuracy)\n",
    "    # find the index of this best lmd\n",
    "    best_lmd_index = np.argmax(accuracy_history)\n",
    "    # find it in the lmd array\n",
    "    lmd_best = lmd[best_lmd_index]\n",
    "    return lmd_best\n",
    "\n",
    "w = np.transpose(np.matrix(np.zeros(np.shape(X)[1])))\n",
    "lmd_best = cross_validation(X, training_set_target, w, 1)\n",
    "lmd_best, len(accuracy_history), accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 300],\n",
       " [300, 600],\n",
       " [600, 900],\n",
       " [900, 1200],\n",
       " [1200, 1500],\n",
       " [1500, 1800],\n",
       " [1800, 2100],\n",
       " [2100, 2400],\n",
       " [2400, 2700],\n",
       " [2700, 3000]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_split = []\n",
    "total=3000;\n",
    "patch=300;\n",
    "for i in range(0,10):\n",
    "    test_split.append([i*300, (i+1)*300]);\n",
    "test_split    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020202020202020204"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmd = np.linspace(0, 1, num=100)\n",
    "lmd[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
