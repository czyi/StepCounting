{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape is:  (4743, 2)\n",
      "test_input shape is:  (1701, 2)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "train_input = pd.read_csv('train.csv')\n",
    "test_input = pd.read_csv('test.csv')\n",
    "print(\"train_input shape is: \", np.shape(train_input))\n",
    "print(\"test_input shape is: \", np.shape(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            handle                                              tweet\n",
      "0   HillaryClinton  The question in this election: Who can put the...\n",
      "1   HillaryClinton  Last night, Donald Trump said not paying taxes...\n",
      "2   HillaryClinton  If we stand together, there's nothing we can't...\n",
      "3   HillaryClinton  Both candidates were asked about how they'd co...\n",
      "4  realDonaldTrump  Join me for a 3pm rally - tomorrow at the Mid-...\n",
      "5   HillaryClinton  When Donald Trump goes low...register to vote:...\n",
      "6   HillaryClinton  3) Has Trump offered a single proposal to redu...\n",
      "7   HillaryClinton  The election is just weeks away. Check if you'...\n",
      "8  realDonaldTrump  Hillary Clinton's Campaign Continues To Make F...\n",
      "9  realDonaldTrump  'CNBC, Time magazine online polls say Donald T...\n"
     ]
    }
   ],
   "source": [
    "print(train_input.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "def label_tweet(input_set):\n",
    "    handle = input_set['handle']\n",
    "    # put it into an array named label, where 0 represents HillaryClinton, 1 represents readDonaldTrump\n",
    "    label = []\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == \"HillaryClinton\":\n",
    "            label.append(0)\n",
    "        if handle[i] == \"realDonaldTrump\":\n",
    "            label.append(1)\n",
    "    label = np.asarray(label)\n",
    "    return label\n",
    "\n",
    "train_label = label_tweet(train_input)\n",
    "print(train_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4743,)\n",
      "[ 'The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG'\n",
      " 'Last night, Donald Trump said not paying taxes was \"smart.\" You know what I call it? Unpatriotic. https://t.co/t0xmBfj7zF'\n",
      " \"If we stand together, there's nothing we can't do. \\n\\nMake sure you're ready to vote: https://t.co/tTgeqxNqYm https://t.co/Q3Ymbb7UNy\"\n",
      " \"Both candidates were asked about how they'd confront racial injustice. Only one had a real answer. https://t.co/sjnEokckis\"\n",
      " 'Join me for a 3pm rally - tomorrow at the Mid-America Center in Council Bluffs, Iowa! Tickets:… https://t.co/dfzsbICiXc'\n",
      " 'When Donald Trump goes low...register to vote: https://t.co/tTgeqxNqYm https://t.co/DXz9dEwsZS'\n",
      " '3) Has Trump offered a single proposal to reduce the friction of starting a business. @HillaryClinton has https://t.co/OhFAFEFsUq'\n",
      " \"The election is just weeks away. Check if you're registered to vote at https://t.co/HcMAh8ljR0, only takes a few cl… https://t.co/H1H7hAA4XM\"\n",
      " \"Hillary Clinton's Campaign Continues To Make False Claims About Foundation Disclosure: \\nhttps://t.co/zhkEfUouHH\"\n",
      " \"'CNBC, Time magazine online polls say Donald Trump won the first presidential debate' via @WashTimes. #MAGA\\nhttps://t.co/PGimqYKPoJ\"]\n",
      "(1701,)\n"
     ]
    }
   ],
   "source": [
    "train_corpus = train_input['tweet'].as_matrix()\n",
    "print(np.shape(train_corpus))\n",
    "print(train_corpus[:10])\n",
    "test_corpus = test_input['tweet'].as_matrix()\n",
    "print(np.shape(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'https']\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 'https' seems useless, so I add it to stop_words\n",
    "stop_words.append(u'https')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "['question election put plans action make life better', 'last night donald trump said paying taxes smart know call unpatriotic', \"stand together 's nothing ca n't make sure 're ready vote\", \"candidates asked 'd confront racial injustice one real answer\", 'join 3pm rally tomorrow mid-america center council bluffs iowa tickets', 'donald trump goes low register vote', 'trump offered single proposal reduce friction starting business hillaryclinton', \"election weeks away check 're registered vote takes cl…\", \"hillary clinton 's campaign continues make false claims foundation disclosure\", \"'cnbc time magazine online polls say donald trump first presidential debate via washtimes maga\"]\n",
      "1701\n",
      "[\"could n't proud hillaryclinton vision command last night 's debate showed 's ready next potus\", \"election important sit go make sure 're registered nationalvoterregistrationday -h\", 'government people join movement today', \"national voterregistrationday make sure 're registered vote makeamericagreatagain…\", 'great afternoon little havana hispanic community leaders thank support imwithyou', \"'s nationalvoterregistrationday celebrate registering vote\", 'love country proud country want leader brings people together —hillary lovetrumpshate', 'kind person would want root million families losing homes one never president —hillary trump', 'think family pay income child care. —hillary', \"join hillary live nc first rally since winning last night 's debate nationalvoterregistrationday\"]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenization(text):\n",
    "    tokens=[]\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        # skip all the websites, punctuations, pure digits\n",
    "        if not re.match('[//]', word) and re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "# Tokenize training set\n",
    "train_corpus_tokenized = []\n",
    "for i in train_corpus:\n",
    "    train_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "    \n",
    "print(len(train_corpus_tokenized))\n",
    "print(train_corpus_tokenized[:10])\n",
    "\n",
    "# Tokenize testing set\n",
    "test_corpus_tokenized = []\n",
    "for i in test_corpus:\n",
    "    test_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "print(len(test_corpus_tokenized))\n",
    "print(test_corpus_tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[['question', 'election', 'put', 'plans', 'action', 'make', 'life', 'better'], ['last', 'night', 'donald', 'trump', 'said', 'paying', 'taxes', 'smart', 'know', 'call', 'unpatriotic'], ['stand', 'together', \"'s\", 'nothing', 'ca', \"n't\", 'make', 'sure', \"'re\", 'ready', 'vote'], ['candidates', 'asked', \"'d\", 'confront', 'racial', 'injustice', 'one', 'real', 'answer'], ['join', '3pm', 'rally', 'tomorrow', 'mid-america', 'center', 'council', 'bluffs', 'iowa', 'tickets'], ['donald', 'trump', 'goes', 'low', 'register', 'vote'], ['trump', 'offered', 'single', 'proposal', 'reduce', 'friction', 'starting', 'business', 'hillaryclinton'], ['election', 'weeks', 'away', 'check', \"'re\", 'registered', 'vote', 'takes', 'cl…'], ['hillary', 'clinton', \"'s\", 'campaign', 'continues', 'make', 'false', 'claims', 'foundation', 'disclosure'], [\"'cnbc\", 'time', 'magazine', 'online', 'polls', 'say', 'donald', 'trump', 'first', 'presidential', 'debate', 'via', 'washtimes', 'maga'], ['donald', 'trump', 'lied', 'american', 'people', 'least', 'times', 'first', 'presidential', 'debate', 'counted'], ['last', 'hrs', 'raised', '13m', 'online', 'donations', 'national', 'call', 'day', 'still', 'going', 'thank', 'america', 'maga'], ['gained', 'pounds', 'months', 'like', 'eating', 'machine.', '—trump', 'man', 'wants', 'president'], ['want', 'turn', 'want', 'work', 'one', 'another', 'want', 'set', 'big', 'goals', 'country', 'strongertogether'], ['hear', 'opponent', 'dangerously', 'incoherent', \"'s\", 'unclear', \"'s\", 'saying', 'words', 'matter', '—hillary'], ['one', 'candidate', 'made', 'clear', 'prepared', 'last', 'night', 'debate', 'made', 'clear', 'prepared', 'b…'], ['really', 'glad', 'dad', 'never', 'contract', 'donald', 'trump.', '—hillary'], ['work', 'hard', 'living', 'poverty'], ['well', \"'re\", 'saying', 'nbc', 'presidential', 'forum', 'last', 'night', 'big', 'debate', 'nice'], ['may', 'record-setting', 'turnout', 'election', 'could', 'biggest', 'turnout', \"'ve\", 'ever']]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "for i in range(len(train_corpus_tokenized)):\n",
    "    train.append(tf.compat.as_str(train_corpus_tokenized[i]).split())\n",
    "print(len(train))\n",
    "print(train[:20])\n",
    "\n",
    "vocabulary_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8507\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "for i in range(len(train)):\n",
    "    for word in train[i]:\n",
    "        cnt[word] += 1\n",
    "\n",
    "print(len(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(cnt, words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(cnt.most_common(n_words - 1))\n",
    "#     print count[:20]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    data_all = []\n",
    "    unk_count = 0\n",
    "    for i in range(len(words)):\n",
    "        inner_data = []\n",
    "        for word in words[i]:\n",
    "            index = dictionary.get(word, 0)\n",
    "            if index == 0:  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            inner_data.append(index)\n",
    "            data_all.append(index)\n",
    "        data.append(inner_data)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, data_all, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, data_all, count, dictionary, reverse_dictionary = build_dataset(cnt, train, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[[973, 104, 124, 415, 490, 12, 150, 89], [44, 83, 7, 1, 47, 732, 491, 278, 59, 353, 3740], [158, 58, 2, 132, 62, 6, 12, 145, 75, 199, 22], [396, 397, 374, 2019, 1437, 1087, 17, 139, 733], [43, 974, 179, 105, 3741, 734, 1653, 3742, 94, 279]]\n",
      "8508\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[:5])\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'piece'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "# print(reverse_dictionary)\n",
    "reverse_dictionary[1080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[[973, 104, 124, 415, 490, 12, 150, 89], [44, 83, 7, 1, 47, 732, 491, 278, 59, 353, 3740], [158, 58, 2, 132, 62, 6, 12, 145, 75, 199, 22], [396, 397, 374, 2019, 1437, 1087, 17, 139, 733], [43, 974, 179, 105, 3741, 734, 1653, 3742, 94, 279]]\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# def align(data):\n",
    "#     max_len = 0\n",
    "#     for row in data:\n",
    "#         if len(row) > max_len:\n",
    "#             max_len = len(row)\n",
    "#     ret = []\n",
    "#     for row in data:\n",
    "#         ret += [row + [0]*(max_len-len(row))]\n",
    "#     return ret, max_len\n",
    "\n",
    "# train_x, seq_max_len = align(data)\n",
    "train_x = data\n",
    "print(len(train_x))\n",
    "print(train_x[:5])\n",
    "print(seq_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[[0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "train_y = [[i, 1-i] for i in train_label]\n",
    "print(len(train_y))\n",
    "print(train_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = len(dictionary),\n",
    "    state_size = 64,\n",
    "    batch_size = 256,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.constant(1.0)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    \"\"\"\n",
    "    Obtain the last relevant output. The best approach in the future will be to use:\n",
    "\n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    which is the Tensorflow equivalent of numpy's rnn_outputs[range(30), seqlen-1, :], but the\n",
    "    gradient for this op has not been implemented as of this writing.\n",
    "\n",
    "    The below solution works, but throws a UserWarning re: the gradient.\n",
    "    \"\"\"\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "batch_start = 0;\n",
    "train_length = len(train_x)\n",
    "train_x.extend(train_x[0:batch_size])\n",
    "train_y.extend(train_y[0:batch_size])\n",
    "training_steps = 6001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "[[973, 104, 124, 415, 490, 12, 150, 89, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [44, 83, 7, 1, 47, 732, 491, 278, 59, 353, 3740, 0, 0, 0, 0, 0, 0, 0], [158, 58, 2, 132, 62, 6, 12, 145, 75, 199, 22, 0, 0, 0, 0, 0, 0, 0], [396, 397, 374, 2019, 1437, 1087, 17, 139, 733, 0, 0, 0, 0, 0, 0, 0, 0, 0], [43, 974, 179, 105, 3741, 734, 1653, 3742, 94, 279, 0, 0, 0, 0, 0, 0, 0, 0], [7, 1, 975, 398, 524, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1654, 1222, 1438, 1655, 3743, 1088, 125, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0], [104, 1223, 558, 492, 75, 600, 22, 439, 3744, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 15, 2, 35, 976, 12, 311, 2579, 735, 1656, 0, 0, 0, 0, 0, 0, 0, 0], [3745, 30, 1657, 1224, 177, 71, 7, 1, 53, 126, 99, 291, 3746, 146, 0, 0, 0, 0]]\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "def get_align_data(data, start, batch_size):\n",
    "    max_len = 0\n",
    "    for row in data[start:start+batch_size]:\n",
    "        if len(row) > max_len:\n",
    "            max_len = len(row)\n",
    "    ret = []\n",
    "    for row in data[start:start+batch_size]:\n",
    "        ret += [row + [0]*(max_len-len(row))]\n",
    "    return ret, max_len\n",
    "\n",
    "first_batch, first_batch_len = get_align_data(train_x, 0, batch_size)\n",
    "print(len(first_batch))\n",
    "print(first_batch[0:10])\n",
    "print(first_batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 256, num_epochs = 10):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        start = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch_x, batch_y, seqlen = get_align_data(train_x, start, batch_size)\n",
    "            feed = {g['x']: batch_x, g['y']: train_y[start:start+batch_size], g['seqlen']: seqlen, g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
