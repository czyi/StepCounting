{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape is:  (4743, 2)\n",
      "test_input shape is:  (1701, 2)\n"
     ]
    }
   ],
   "source": [
    "train_input = pd.read_csv('train.csv')\n",
    "test_input = pd.read_csv('test.csv')\n",
    "print \"train_input shape is: \", np.shape(train_input)\n",
    "print \"test_input shape is: \", np.shape(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_tweet(input_set):\n",
    "    handle = input_set['handle']\n",
    "    # put it into an array named label, \n",
    "    # where 0 represents HillaryClinton, \n",
    "    # 1 represents readDonaldTrump\n",
    "    label = []\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == \"HillaryClinton\":\n",
    "            label.append(0)\n",
    "        if handle[i] == \"realDonaldTrump\":\n",
    "            label.append(1)\n",
    "    label = np.asarray(label)\n",
    "    return label\n",
    "\n",
    "train_label = label_tweet(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = train_input['tweet'].as_matrix()\n",
    "test_corpus = test_input['tweet'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 'https' seems useless, so I add it to stop_words\n",
    "stop_words.append(u'https')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization, training set and testing set look like:\n",
      "[u'question election put plans action make life better', u'last night donald trump said paying taxes smart know call unpatriotic', u\"stand together 's nothing ca n't make sure 're ready vote\", u\"candidates asked 'd confront racial injustice one real answer\", u'join 3pm rally tomorrow mid-america center council bluffs iowa tickets']\n",
      "[u\"could n't proud hillaryclinton vision command last night 's debate showed 's ready next potus\", u\"election important sit go make sure 're registered nationalvoterregistrationday -h\", u'government people join movement today', u\"national voterregistrationday make sure 're registered vote makeamericagreatagain\\u2026\", u'great afternoon little havana hispanic community leaders thank support imwithyou']\n"
     ]
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "#     return [i for i in text]\n",
    "    tokens=[]\n",
    "    for word in nltk.word_tokenize(text.decode('utf-8')):\n",
    "        # skip all the websites, punctuations, pure digits\n",
    "        if not re.match('[//]', word) and re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "#             tokens.extend([i for i in word.lower()])\n",
    "    return tokens\n",
    "\n",
    "# Tokenize training set\n",
    "train_corpus_tokenized = []\n",
    "for i in train_corpus:\n",
    "    train_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "# Tokenize testing set\n",
    "test_corpus_tokenized = []\n",
    "for i in test_corpus:\n",
    "    test_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "print \"After tokenization, training set and testing set look like:\"\n",
    "print(train_corpus_tokenized[:5])\n",
    "print(test_corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tf.compat.as_str, training set and testing set look like:\n",
      "[['question', 'election', 'put', 'plans', 'action', 'make', 'life', 'better'], ['last', 'night', 'donald', 'trump', 'said', 'paying', 'taxes', 'smart', 'know', 'call', 'unpatriotic'], ['stand', 'together', \"'s\", 'nothing', 'ca', \"n't\", 'make', 'sure', \"'re\", 'ready', 'vote'], ['candidates', 'asked', \"'d\", 'confront', 'racial', 'injustice', 'one', 'real', 'answer'], ['join', '3pm', 'rally', 'tomorrow', 'mid-america', 'center', 'council', 'bluffs', 'iowa', 'tickets']]\n",
      "[['could', \"n't\", 'proud', 'hillaryclinton', 'vision', 'command', 'last', 'night', \"'s\", 'debate', 'showed', \"'s\", 'ready', 'next', 'potus'], ['election', 'important', 'sit', 'go', 'make', 'sure', \"'re\", 'registered', 'nationalvoterregistrationday', '-h'], ['government', 'people', 'join', 'movement', 'today'], ['national', 'voterregistrationday', 'make', 'sure', \"'re\", 'registered', 'vote', 'makeamericagreatagain\\xe2\\x80\\xa6'], ['great', 'afternoon', 'little', 'havana', 'hispanic', 'community', 'leaders', 'thank', 'support', 'imwithyou']]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_word = []\n",
    "for i in range(len(train_corpus_tokenized)):\n",
    "    train_tokenized_word.append(tf.compat.as_str(train_corpus_tokenized[i]).split())\n",
    "\n",
    "test_tokenized_word = []\n",
    "for i in range(len(test_corpus_tokenized)):\n",
    "    test_tokenized_word.append(tf.compat.as_str(test_corpus_tokenized[i]).split())\n",
    "    \n",
    "print \"After tf.compat.as_str, training set and testing set look like:\"\n",
    "print(train_tokenized_word[:5])\n",
    "print(test_tokenized_word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altogether there are: 8507 words\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "for i in range(len(train_tokenized_word)):\n",
    "    for word in train_tokenized_word[i]:\n",
    "        cnt[word] += 1\n",
    "\n",
    "print 'Altogether there are: ' + str((len(cnt))) + ' words'\n",
    "\n",
    "vocabulary_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(cnt, words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(cnt.most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        inner_data = []\n",
    "        for word in words[i]:\n",
    "            index = dictionary.get(word, 0)\n",
    "            if index == 0:  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            inner_data.append(index)\n",
    "        data.append(inner_data)\n",
    "        \n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "train_x, count, dictionary, reverse_dictionary = build_dataset(cnt, train_tokenized_word, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data set size is: 1701, such as:\n",
      "[[[92, 6, 123, 56, 1097, 0, 44, 83, 2, 101, 1224, 2, 200, 160, 32], [0, 0]], [[108, 271, 2004, 50, 12, 148, 76, 616, 1765, 81], [0, 0]], [[1276, 9, 43, 212, 27], [0, 0]], [[118, 0, 12, 148, 76, 616, 22, 0], [0, 0]], [[5, 933, 244, 0, 2910, 606, 517, 4, 66, 168], [0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "# Process testing data\n",
    "test = []\n",
    "for sentence in test_tokenized_word:\n",
    "    cur = []\n",
    "    for word in sentence:\n",
    "        if(word in dictionary):\n",
    "            cur.append(dictionary[word])\n",
    "        else:\n",
    "            cur.append(0)\n",
    "    # to store corresponding label\n",
    "    test.append([cur,[0, 0]])\n",
    "\n",
    "test_length = len(test)\n",
    "print \"Testing data set size is: \" + str((len(test))) + \", such as:\"\n",
    "print(test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set size is: 3794, such as:\n",
      "[[[8, 1382, 324, 65, 39, 838], [1, 0]], [[680, 67, 1771, 847, 1961, 9, 84, 371, 522, 24, 2, 236], [0, 1]], [[713, 163, 184, 1783, 45, 71, 213, 198, 2434, 11, 667], [0, 1]], [[6453, 2655, 170, 1953, 954, 1010, 5850, 16], [0, 1]], [[443, 1652, 320, 1, 20, 127, 23, 1902, 1152], [0, 1]]]\n",
      "Testing data set size is: 949, such as:\n",
      "[[[14, 507, 1078], [1, 0]], [[7, 1, 2, 775, 941, 86, 119, 256, 145, 775, 941, 761], [0, 1]], [[6691, 14], [1, 0]], [[331, 149, 179, 8, 103, 25, 19, 200, 7, 76, 508], [0, 1]], [[459, 3016, 190, 99, 34, 828, 308, 159, 457, 1, 821, 13, 1394, 167, 692], [1, 0]]]\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "train_all = [[train_x[i], [train_label[i], 1-train_label[i]]] for i in range(0, len(train_x))]\n",
    "\n",
    "# shuffle the training set in which to pick training and validation sets\n",
    "r_index = list(range(len(train_all)))\n",
    "random.shuffle(r_index)\n",
    "train = [train_all[i] for i in r_index[:int(len(r_index)*0.8)]]\n",
    "valid = [train_all[i] for i in r_index[int(len(r_index)*0.8):]]\n",
    "\n",
    "print \"Training data set size is: \" + str((len(train))) + \", such as:\"\n",
    "print(train[:5])\n",
    "print \"Testing data set size is: \" + str((len(valid))) + \", such as:\"\n",
    "print(valid[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.df)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n > self.size:\n",
    "            self.epochs += 1\n",
    "            print(\"SimpleDataIterator epoch : \", self.epochs)\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor : self.cursor + n]\n",
    "        self.cursor += n\n",
    "        return res\n",
    "\n",
    "# pad vectors in the same batch to keep their lengths identical\n",
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor : self.cursor + n]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        max_len = 0\n",
    "        for row in res:\n",
    "            if len(row[0]) > max_len:\n",
    "                max_len = len(row[0])\n",
    "        seqlen = np.array([max_len for i in range(len(res))])\n",
    "        ret = []\n",
    "        label = []\n",
    "        for row in res:\n",
    "            ret += [row[0] + [0] * (max_len - len(row[0]))]\n",
    "            label.append(row[1][0])\n",
    "        x = np.array(ret)\n",
    "        y = np.array(label)\n",
    "\n",
    "        return x, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences is like this:\n",
      "[[[37, 1413, 874, 2666, 1862, 7708, 1862, 1733, 693], [0, 1]], [[4, 120, 227, 498, 426, 528, 21, 1484], [1, 0]], [[1827, 735, 994, 3406, 7, 1, 67, 192, 202, 522, 522], [0, 1]], [[2, 54, 1644, 23, 1239, 23, 2, 30, 184, 58, 114, 7, 1], [0, 1]], [[2532, 2196, 229, 8313, 2178, 943, 235, 287, 826, 27, 60, 51, 6126, 81], [0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(valid)\n",
    "d = data.next_batch(500)\n",
    "print 'Input sequences is like this:' \n",
    "print d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences in one random batch is now like:\n",
      "[[4668    8 8431    7   19   36  126   39   12   10    5    0    0    0]\n",
      " [  93 3210 2871  524  482 6282   32    2   80    0    0    0    0    0]\n",
      " [  53  366   11   12  516  983   18 1147   65  268  105  325 8368   16]]\n",
      "with shape of: \n",
      "(3, 14)\n",
      "Corresponding labels are: \n",
      "[1 1 0]\n",
      "where 0 stands for Hillary and 1 for Trump.\n"
     ]
    }
   ],
   "source": [
    "data = PaddedDataIterator(train)\n",
    "d = data.next_batch(3)\n",
    "print 'Input sequences in one random batch is now like:'\n",
    "print d[0]\n",
    "print 'with shape of: '\n",
    "print d[0].shape\n",
    "print 'Corresponding labels are: '\n",
    "print d[1]\n",
    "print 'where 0 stands for Hillary and 1 for Trump.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set length = 1701\n",
      "batch size = 256\n",
      "so there should be 7 batches\n",
      " \n",
      "testing batch 1 complete, with the vector length equals 19\n",
      "testing batch 2 complete, with the vector length equals 18\n",
      "testing batch 3 complete, with the vector length equals 23\n",
      "testing batch 4 complete, with the vector length equals 20\n",
      "testing batch 5 complete, with the vector length equals 20\n",
      "testing batch 6 complete, with the vector length equals 18\n",
      "testing batch 7 complete, with the vector length equals 19\n"
     ]
    }
   ],
   "source": [
    "# do the same thing for the testing set\n",
    "def align(data):\n",
    "    max_len = 0\n",
    "    for row in data:\n",
    "        if len(row[0]) > max_len:\n",
    "            max_len = len(row[0])\n",
    "    ret = []\n",
    "    label = []\n",
    "    for row in data:\n",
    "        ret += [row[0] + [0]*(max_len - len(row[0]))]\n",
    "        label.append(row[1][0])\n",
    "    x = np.array(ret)\n",
    "    y = np.array(label)\n",
    "    seq_len = np.array([max_len for i in data])\n",
    "    \n",
    "    return x, y, seq_len\n",
    "\n",
    "batch_size = 256\n",
    "print 'test set length = %d' % test_length\n",
    "print 'batch size = %d' % batch_size\n",
    "print 'so there should be %d batches' % (test_length / batch_size + 1)\n",
    "print ' '\n",
    "\n",
    "test_list = []\n",
    "test_addlen = test\n",
    "test_addlen.extend(test[0:batch_size])\n",
    "for i in range(test_length / batch_size + 1):\n",
    "    x, y, seq_len = align(test[i * batch_size : (i+1) * batch_size])\n",
    "    print 'testing batch ' + str(i + 1) + ' complete, with the vector length equals ' + str(x.shape[1])\n",
    "    test_list.append([x, y, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "# Build RNN model\n",
    "def build_graph(\n",
    "    vocab_size = len(dictionary),\n",
    "    state_size = 32,\n",
    "    batch_size = 256,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # We dont' have to add dropout\n",
    "    # rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    # Obtain the last relevant output\n",
    "    idx = tf.range(batch_size) * tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # finally use a Softmax layer to output a probability\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    \n",
    "    # evaluate the model\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "   \n",
    "    # optimizer, which could be tuned\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(g, batch_size = 256, num_epochs = 15, iterator = PaddedDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        tv = iterator(valid)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, tv_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                # evaluate validation set\n",
    "                tv_epoch = tv.epochs\n",
    "                while tv.epochs == tv_epoch:\n",
    "                    step += 1\n",
    "                    batch = tv.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                tv_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print 'Accuracy after epoch %d is: ' % current_epoch \n",
    "                print 'training: %f, validation: %f' % (tr_losses[-1], tv_losses[-1])\n",
    "        print '---------- RNN training done! ----------'\n",
    "            \n",
    "        # make predictions with the current model\n",
    "        predictions = []\n",
    "        for te in test_list:\n",
    "            feed = {g['x']: te[0], g['y']: te[1], g['seqlen']: te[2]}\n",
    "            preds_, _ = sess.run([g['preds'], g['ts']], feed_dict=feed)\n",
    "            predictions.extend(preds_)\n",
    "\n",
    "    return tr_losses, tv_losses, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1 is: \n",
      "training: 0.509115, validation: 0.490234\n",
      "Accuracy after epoch 2 is: \n",
      "training: 0.505301, validation: 0.486979\n",
      "Accuracy after epoch 3 is: \n",
      "training: 0.505301, validation: 0.510417\n",
      "Accuracy after epoch 4 is: \n",
      "training: 0.520368, validation: 0.496094\n",
      "Accuracy after epoch 5 is: \n",
      "training: 0.532645, validation: 0.507812\n",
      "Accuracy after epoch 6 is: \n",
      "training: 0.537946, validation: 0.566406\n",
      "Accuracy after epoch 7 is: \n",
      "training: 0.648158, validation: 0.656250\n",
      "Accuracy after epoch 8 is: \n",
      "training: 0.818359, validation: 0.789062\n",
      "Accuracy after epoch 9 is: \n",
      "training: 0.907087, validation: 0.880208\n",
      "Accuracy after epoch 10 is: \n",
      "training: 0.922991, validation: 0.871094\n",
      "Accuracy after epoch 11 is: \n",
      "training: 0.933036, validation: 0.895833\n",
      "Accuracy after epoch 12 is: \n",
      "training: 0.936663, validation: 0.890625\n",
      "Accuracy after epoch 13 is: \n",
      "training: 0.945312, validation: 0.897135\n",
      "Accuracy after epoch 14 is: \n",
      "training: 0.946429, validation: 0.912760\n",
      "Accuracy after epoch 15 is: \n",
      "training: 0.947824, validation: 0.893229\n",
      "---------- RNN training done! ----------\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, tv_losses, predictions = train_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions are like: \n",
      "[array([ 0.61423916,  0.38576075], dtype=float32), array([ 0.66434556,  0.33565447], dtype=float32), array([ 0.43157271,  0.56842732], dtype=float32), array([ 0.57348108,  0.42651895], dtype=float32), array([ 0.35469803,  0.64530194], dtype=float32), array([ 0.5702135,  0.4297865], dtype=float32), array([ 0.57474524,  0.4252547 ], dtype=float32), array([ 0.65039474,  0.34960526], dtype=float32), array([ 0.59814698,  0.40185308], dtype=float32), array([ 0.53202361,  0.46797639], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print 'Some predictions are like: '\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = file('csvtest11.csv', 'wb')\n",
    "writer = csv.writer(csvfile)\n",
    "writer.writerow(['id', 'realDonaldTrump', 'HillaryClinton'])\n",
    "data = []\n",
    "for i in range(test_length):\n",
    "    data.append((i, predictions[i][1], predictions[i][0]))\n",
    "\n",
    "writer.writerows(data)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
