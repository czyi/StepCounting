{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape is:  (4743, 2)\n",
      "test_input shape is:  (1701, 2)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "train_input = pd.read_csv('train.csv')\n",
    "test_input = pd.read_csv('test.csv')\n",
    "print(\"train_input shape is: \", np.shape(train_input))\n",
    "print(\"test_input shape is: \", np.shape(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "def label_tweet(input_set):\n",
    "    handle = input_set['handle']\n",
    "    # put it into an array named label, where 0 represents HillaryClinton, 1 represents readDonaldTrump\n",
    "    label = []\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == \"HillaryClinton\":\n",
    "            label.append(0)\n",
    "        if handle[i] == \"realDonaldTrump\":\n",
    "            label.append(1)\n",
    "    label = np.asarray(label)\n",
    "    return label\n",
    "\n",
    "train_label = label_tweet(train_input)\n",
    "print(train_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4743,)\n",
      "[ 'The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG'\n",
      " 'Last night, Donald Trump said not paying taxes was \"smart.\" You know what I call it? Unpatriotic. https://t.co/t0xmBfj7zF'\n",
      " \"If we stand together, there's nothing we can't do. \\n\\nMake sure you're ready to vote: https://t.co/tTgeqxNqYm https://t.co/Q3Ymbb7UNy\"\n",
      " \"Both candidates were asked about how they'd confront racial injustice. Only one had a real answer. https://t.co/sjnEokckis\"\n",
      " 'Join me for a 3pm rally - tomorrow at the Mid-America Center in Council Bluffs, Iowa! Tickets:… https://t.co/dfzsbICiXc']\n",
      "(1701,)\n"
     ]
    }
   ],
   "source": [
    "train_corpus = train_input['tweet'].as_matrix()\n",
    "print(np.shape(train_corpus))\n",
    "print(train_corpus[:5])\n",
    "test_corpus = test_input['tweet'].as_matrix()\n",
    "print(np.shape(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'https']\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 'https' seems useless, so I add it to stop_words\n",
    "stop_words.append(u'https')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "['question election put plans action make life better', 'last night donald trump said paying taxes smart know call unpatriotic', \"stand together 's nothing ca n't make sure 're ready vote\", \"candidates asked 'd confront racial injustice one real answer\", 'join 3pm rally tomorrow mid-america center council bluffs iowa tickets']\n",
      "1701\n",
      "[\"could n't proud hillaryclinton vision command last night 's debate showed 's ready next potus\", \"election important sit go make sure 're registered nationalvoterregistrationday -h\", 'government people join movement today', \"national voterregistrationday make sure 're registered vote makeamericagreatagain…\", 'great afternoon little havana hispanic community leaders thank support imwithyou']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenization(text):\n",
    "#     return [i for i in text]\n",
    "    tokens=[]\n",
    "#     for word in nltk.word_tokenize(text.decode('utf-8')):\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        # skip all the websites, punctuations, pure digits\n",
    "        if not re.match('[//]', word) and re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "# Tokenize training set\n",
    "train_corpus_tokenized = []\n",
    "for i in train_corpus:\n",
    "    train_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "    \n",
    "print(len(train_corpus_tokenized))\n",
    "print(train_corpus_tokenized[:5])\n",
    "\n",
    "# Tokenize testing set\n",
    "test_corpus_tokenized = []\n",
    "for i in test_corpus:\n",
    "    test_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "print(len(test_corpus_tokenized))\n",
    "print(test_corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[['question', 'election', 'put', 'plans', 'action', 'make', 'life', 'better'], ['last', 'night', 'donald', 'trump', 'said', 'paying', 'taxes', 'smart', 'know', 'call', 'unpatriotic'], ['stand', 'together', \"'s\", 'nothing', 'ca', \"n't\", 'make', 'sure', \"'re\", 'ready', 'vote'], ['candidates', 'asked', \"'d\", 'confront', 'racial', 'injustice', 'one', 'real', 'answer'], ['join', '3pm', 'rally', 'tomorrow', 'mid-america', 'center', 'council', 'bluffs', 'iowa', 'tickets']]\n",
      "1701\n",
      "[['could', \"n't\", 'proud', 'hillaryclinton', 'vision', 'command', 'last', 'night', \"'s\", 'debate', 'showed', \"'s\", 'ready', 'next', 'potus'], ['election', 'important', 'sit', 'go', 'make', 'sure', \"'re\", 'registered', 'nationalvoterregistrationday', '-h'], ['government', 'people', 'join', 'movement', 'today'], ['national', 'voterregistrationday', 'make', 'sure', \"'re\", 'registered', 'vote', 'makeamericagreatagain…'], ['great', 'afternoon', 'little', 'havana', 'hispanic', 'community', 'leaders', 'thank', 'support', 'imwithyou']]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_word = []\n",
    "for i in range(len(train_corpus_tokenized)):\n",
    "    train_tokenized_word.append(tf.compat.as_str(train_corpus_tokenized[i]).split())\n",
    "print(len(train_tokenized_word))\n",
    "print(train_tokenized_word[:5])\n",
    "\n",
    "vocabulary_size = 10000\n",
    "\n",
    "test_tokenized_word = []\n",
    "for i in range(len(test_corpus_tokenized)):\n",
    "    test_tokenized_word.append(tf.compat.as_str(test_corpus_tokenized[i]).split())\n",
    "print(len(test_tokenized_word))\n",
    "print(test_tokenized_word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8507\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "for i in range(len(train_tokenized_word)):\n",
    "    for word in train_tokenized_word[i]:\n",
    "        cnt[word] += 1\n",
    "\n",
    "print(len(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(cnt, words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(cnt.most_common(n_words - 1))\n",
    "#     print count[:20]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    data_all = []\n",
    "    unk_count = 0\n",
    "    for i in range(len(words)):\n",
    "        inner_data = []\n",
    "        for word in words[i]:\n",
    "            index = dictionary.get(word, 0)\n",
    "            if index == 0:  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            inner_data.append(index)\n",
    "            data_all.append(index)\n",
    "        data.append(inner_data)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, data_all, count, dictionary, reversed_dictionary\n",
    "\n",
    "train_x, data_all, count, dictionary, reverse_dictionary = build_dataset(cnt, train_tokenized_word, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n",
      "[[[92, 6, 120, 56, 1150, 0, 44, 83, 2, 99, 1387, 2, 199, 162, 32], [0, 0]], [[104, 271, 1683, 51, 12, 145, 75, 600, 1661, 82], [0, 0]], [[1341, 9, 43, 207, 26], [0, 0]], [[116, 0, 12, 145, 75, 600, 22, 0], [0, 0]], [[5, 914, 241, 0, 2898, 622, 497, 4, 68, 169], [0, 0]]]\n",
      "1701\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for sentence in test_tokenized_word:\n",
    "    cur = []\n",
    "    for word in sentence:\n",
    "        if(word in dictionary):\n",
    "            cur.append(dictionary[word])\n",
    "        else:\n",
    "            cur.append(0)\n",
    "    test.append([cur,[0, 0]])\n",
    "\n",
    "print(len(test))\n",
    "print(test[:5])\n",
    "test_length = len(test)\n",
    "print(test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3794\n",
      "[[[25, 9, 6517, 1268, 1, 364, 71, 1018, 326], [0, 1]], [[32, 325, 1945, 736, 64, 860, 45, 23, 57, 16], [0, 1]], [[1920, 2, 195, 25, 33, 41, 856, 3264, 23, 9, 16], [0, 1]], [[814, 911, 8, 679, 570, 1749, 2475, 6425, 6426, 6427], [1, 0]], [[424, 33, 2097, 1328, 84, 3371, 1513, 357, 507, 6171, 175, 158, 58], [0, 1]]]\n",
      "949\n",
      "[[[314, 339, 578, 2425, 1406, 1688, 44, 66, 8, 2, 1403], [0, 1]], [[2, 447, 603, 261, 323, 56, 32, 914, 999, 5744], [0, 1]], [[376, 312], [0, 1]], [[127, 7419, 3604, 35, 1957, 2223, 140, 3603, 7420, 964, 103, 7421], [1, 0]], [[7572, 8, 190, 2, 957, 7573, 93, 62, 6, 158], [1, 0]]]\n",
      "8508\n"
     ]
    }
   ],
   "source": [
    "train_all = [[train_x[i], [train_label[i], 1-train_label[i]]] for i in range(0, len(train_x))]\n",
    "# train_all = [[train_x[i], [train_label[i]]] for i in range(0, len(train_x))]\n",
    "\n",
    "r_index = list(range(len(train_all)))\n",
    "random.shuffle(r_index)\n",
    "train = [train_all[i] for i in r_index[:int(len(r_index)*0.8)]]\n",
    "valid = [train_all[i] for i in r_index[int(len(r_index)*0.8):]]\n",
    "\n",
    "print(len(train))\n",
    "print(train[:5])\n",
    "print(len(valid))\n",
    "print(valid[:5])\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.df)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            print(\"SimpleDataIterator epoch : \", self.epochs)\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences :  [[[6176, 8, 601, 12, 23, 171, 3, 11, 55, 1408], [1, 0]], [[640, 5207, 637, 5208, 163, 244, 58, 351], [0, 1]], [[1, 294, 1093, 764, 541, 451, 1488, 823, 149, 4070, 442, 4071], [0, 1]], [[103, 180, 4519, 127, 644], [0, 1]], [[1264, 85, 165, 180, 58, 36, 54, 316, 7498, 77, 33, 5, 1075, 859, 1917], [1, 0]]]\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(valid)\n",
    "d = data.next_batch(500)\n",
    "print('Input sequences : ', d[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        max_len = 0\n",
    "        for row in res:\n",
    "            if len(row[0]) > max_len:\n",
    "                max_len = len(row[0])\n",
    "        seqlen = np.array([max_len for i in range(len(res))])\n",
    "        ret = []\n",
    "        label = []\n",
    "        for row in res:\n",
    "            ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "            label.append(row[1][0])\n",
    "        x = np.array(ret)\n",
    "        y = np.array(label)\n",
    "\n",
    "        return x, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences\n",
      " [[ 587  535  812 1162  779 4325  404  398  385  588  932  818    0    0\n",
      "     0]\n",
      " [1944  481  594 8071   18  189 8072  551 3189  238 8073  371 2567  485\n",
      "   282]\n",
      " [ 480   15  735    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "(3, 15)\n",
      "[1 1 1]\n",
      "(3,)\n",
      "[15 15 15]\n"
     ]
    }
   ],
   "source": [
    "data = PaddedDataIterator(train)\n",
    "d = data.next_batch(3)\n",
    "print('Input sequences\\n', d[0])\n",
    "print(d[0].shape)\n",
    "print(d[1])\n",
    "print(d[1].shape)\n",
    "print(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n",
      "256\n",
      "6.64453125\n",
      "0\n",
      "256\n",
      "19\n",
      "(256, 19) ,  (256,) ,  (256,)\n",
      "1\n",
      "256\n",
      "18\n",
      "(256, 18) ,  (256,) ,  (256,)\n",
      "2\n",
      "256\n",
      "23\n",
      "(256, 23) ,  (256,) ,  (256,)\n",
      "3\n",
      "256\n",
      "20\n",
      "(256, 20) ,  (256,) ,  (256,)\n",
      "4\n",
      "256\n",
      "20\n",
      "(256, 20) ,  (256,) ,  (256,)\n",
      "5\n",
      "256\n",
      "18\n",
      "(256, 18) ,  (256,) ,  (256,)\n",
      "6\n",
      "256\n",
      "19\n",
      "(256, 19) ,  (256,) ,  (256,)\n",
      "=====\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def align(data):\n",
    "    print(len(data))\n",
    "    max_len = 0\n",
    "    for row in data:\n",
    "        if len(row[0]) > max_len:\n",
    "            max_len = len(row[0])\n",
    "    print(max_len)\n",
    "    ret = []\n",
    "    label = []\n",
    "    for row in data:\n",
    "        ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "        label.append(row[1][0])\n",
    "    x = np.array(ret)\n",
    "    y = np.array(label)\n",
    "    seq_len = np.array([max_len for i in data])\n",
    "    \n",
    "    return x, y, seq_len\n",
    "\n",
    "# def xx():\n",
    "#     max_len = 0\n",
    "#     for row in res:\n",
    "#         if len(row[0]) > max_len:\n",
    "#             max_len = len(row[0])\n",
    "#     seqlen = np.array([max_len for i in range(len(res))])\n",
    "#     ret = []\n",
    "#     label = []\n",
    "#     for row in res:\n",
    "#         ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "#         label.append(row[1][0])\n",
    "#     x = np.array(ret)\n",
    "#     y = np.array(label)\n",
    "\n",
    "# test_length = 1701\n",
    "print(test_length)\n",
    "print(batch_size)\n",
    "print(test_length/batch_size)\n",
    "\n",
    "test_list = []\n",
    "test_addlen = test\n",
    "test_addlen.extend(test[0:batch_size])\n",
    "# for i in range(test_length/batch_size+1):\n",
    "for i in range(math.ceil(test_length/batch_size)):\n",
    "    print(i)\n",
    "    x, y, seq_len = align(test[i*batch_size:(i+1)*batch_size])\n",
    "    print(x.shape, \", \", y.shape, \", \", seq_len.shape)\n",
    "#     print(y.shape)\n",
    "#     print(seq_len.shape)\n",
    "    test_list.append([x, y, seq_len])\n",
    "\n",
    "print(\"=====\")\n",
    "print(len(test_list[0]))\n",
    "    \n",
    "\n",
    "# test_align, max_len = align(test)\n",
    "# print(len(test_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "NUM_LAYERS=2\n",
    "\n",
    "def gruCell(state_size):\n",
    "#     lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, state_is_tuple=True)\n",
    "    gru_cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    return gru_cell\n",
    "    \n",
    "def build_graph(\n",
    "    vocab_size = len(dictionary),\n",
    "    state_size = 128,\n",
    "    batch_size = 256,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "#     keep_prob = tf.constant(1.0)\n",
    "#     print(\"====\",x.shape)\n",
    "#     print(\"====\",y.shape)\n",
    "#     print(\"====\",seqlen.shape)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "#     # RNN\n",
    "#     cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "# #     cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "# #     cell = tf.contrib.rnn.MultiRNNCell([gruCell(state_size) for _ in range(NUM_LAYERS)])\n",
    "#     init_state = tf.get_variable('init_state', [1, state_size],\n",
    "#                                  initializer=tf.constant_initializer(0.0))\n",
    "#     init_state = tf.tile(init_state, [batch_size, 1])\n",
    "#     rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, sequence_length=seqlen,\n",
    "#                                                  initial_state=init_state)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "#     rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    \"\"\"\n",
    "    Obtain the last relevant output. The best approach in the future will be to use:\n",
    "\n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    which is the Tensorflow equivalent of numpy's rnn_outputs[range(30), seqlen-1, :], but the\n",
    "    gradient for this op has not been implemented as of this writing.\n",
    "\n",
    "    The below solution works, but throws a UserWarning re: the gradient.\n",
    "    \"\"\"\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "#     print(correct.shape)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "#     print(accuracy.shape)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "#         'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 256, num_epochs = 50, iterator = PaddedDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        tv = iterator(valid)\n",
    "#         te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, tv_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "#             print(\"----\",batch[0].shape)\n",
    "#             print(\"----\",batch[1].shape)\n",
    "#             print(\"----\",batch[2].shape)\n",
    "#             dropout_parameter=np.array(0.6)\n",
    "#             print(\"----\",dropout_parameter.shape)\n",
    "#             feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: dropout_parameter}\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                tv_epoch = tv.epochs\n",
    "                while tv.epochs == tv_epoch:\n",
    "                    step += 1\n",
    "                    batch = tv.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                tv_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- tv:\", tv_losses[-1])\n",
    "            \n",
    "        predictions = []\n",
    "        for te in test_list:\n",
    "            feed = {g['x']: te[0], g['y']: te[1], g['seqlen']: te[2]}\n",
    "            preds_, _ = sess.run([g['preds'], g['ts']], feed_dict=feed)\n",
    "            print(len(preds_))\n",
    "            predictions.extend(preds_)\n",
    "\n",
    "    return tr_losses, tv_losses, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyi/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.508333333333 - tv: 0.4873046875\n",
      "Accuracy after epoch 2  - tr: 0.502511160714 - tv: 0.514322916667\n",
      "Accuracy after epoch 3  - tr: 0.537388392857 - tv: 0.654947916667\n",
      "Accuracy after epoch 4  - tr: 0.871930803571 - tv: 0.83203125\n",
      "Accuracy after epoch 5  - tr: 0.924107142857 - tv: 0.8671875\n",
      "Accuracy after epoch 6  - tr: 0.932756696429 - tv: 0.885416666667\n",
      "Accuracy after epoch 7  - tr: 0.942801339286 - tv: 0.864583333333\n",
      "Accuracy after epoch 8  - tr: 0.946149553571 - tv: 0.888020833333\n",
      "Accuracy after epoch 9  - tr: 0.954520089286 - tv: 0.89453125\n",
      "Accuracy after epoch 10  - tr: 0.967912946429 - tv: 0.890625\n",
      "Accuracy after epoch 11  - tr: 0.976004464286 - tv: 0.90234375\n",
      "Accuracy after epoch 12  - tr: 0.984933035714 - tv: 0.899739583333\n",
      "Accuracy after epoch 13  - tr: 0.990513392857 - tv: 0.91015625\n",
      "Accuracy after epoch 14  - tr: 0.992466517857 - tv: 0.889322916667\n",
      "Accuracy after epoch 15  - tr: 0.994698660714 - tv: 0.90234375\n",
      "Accuracy after epoch 16  - tr: 0.995535714286 - tv: 0.901041666667\n",
      "Accuracy after epoch 17  - tr: 0.996930803571 - tv: 0.88671875\n",
      "Accuracy after epoch 18  - tr: 0.997209821429 - tv: 0.889322916667\n",
      "Accuracy after epoch 19  - tr: 0.998046875 - tv: 0.895833333333\n",
      "Accuracy after epoch 20  - tr: 0.998046875 - tv: 0.88671875\n",
      "Accuracy after epoch 21  - tr: 0.998604910714 - tv: 0.889322916667\n",
      "Accuracy after epoch 22  - tr: 0.998325892857 - tv: 0.899739583333\n",
      "Accuracy after epoch 23  - tr: 0.998883928571 - tv: 0.8984375\n",
      "Accuracy after epoch 24  - tr: 0.998604910714 - tv: 0.8984375\n",
      "Accuracy after epoch 25  - tr: 0.999441964286 - tv: 0.890625\n",
      "Accuracy after epoch 26  - tr: 0.998883928571 - tv: 0.8984375\n",
      "Accuracy after epoch 27  - tr: 0.999162946429 - tv: 0.8984375\n",
      "Accuracy after epoch 28  - tr: 0.999162946429 - tv: 0.881510416667\n",
      "Accuracy after epoch 29  - tr: 0.999162946429 - tv: 0.889322916667\n",
      "Accuracy after epoch 30  - tr: 0.999162946429 - tv: 0.88671875\n",
      "Accuracy after epoch 31  - tr: 0.999162946429 - tv: 0.901041666667\n",
      "Accuracy after epoch 32  - tr: 0.998604910714 - tv: 0.904947916667\n",
      "Accuracy after epoch 33  - tr: 0.999162946429 - tv: 0.903645833333\n",
      "Accuracy after epoch 34  - tr: 0.998883928571 - tv: 0.890625\n",
      "Accuracy after epoch 35  - tr: 0.999162946429 - tv: 0.889322916667\n",
      "Accuracy after epoch 36  - tr: 0.999162946429 - tv: 0.893229166667\n",
      "Accuracy after epoch 37  - tr: 0.998883928571 - tv: 0.87890625\n",
      "Accuracy after epoch 38  - tr: 0.998604910714 - tv: 0.893229166667\n",
      "Accuracy after epoch 39  - tr: 0.999162946429 - tv: 0.889322916667\n",
      "Accuracy after epoch 40  - tr: 0.999162946429 - tv: 0.8828125\n",
      "Accuracy after epoch 41  - tr: 0.998883928571 - tv: 0.8828125\n",
      "Accuracy after epoch 42  - tr: 0.998883928571 - tv: 0.884114583333\n",
      "Accuracy after epoch 43  - tr: 0.999162946429 - tv: 0.889322916667\n",
      "Accuracy after epoch 44  - tr: 0.999441964286 - tv: 0.890625\n",
      "Accuracy after epoch 45  - tr: 0.998604910714 - tv: 0.8828125\n",
      "Accuracy after epoch 46  - tr: 0.999162946429 - tv: 0.893229166667\n",
      "Accuracy after epoch 47  - tr: 0.999162946429 - tv: 0.876302083333\n",
      "Accuracy after epoch 48  - tr: 0.998604910714 - tv: 0.901041666667\n",
      "Accuracy after epoch 49  - tr: 0.999720982143 - tv: 0.8828125\n",
      "Accuracy after epoch 50  - tr: 0.999441964286 - tv: 0.889322916667\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "1792\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, tv_losses, predictions = train_graph(g)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  9.99783456e-01,   2.16587825e-04], dtype=float32), array([  9.99985099e-01,   1.49040734e-05], dtype=float32), array([  4.22566445e-05,   9.99957800e-01], dtype=float32), array([  9.99928355e-01,   7.15992355e-05], dtype=float32), array([  2.82522287e-05,   9.99971747e-01], dtype=float32), array([  9.99980330e-01,   1.97034951e-05], dtype=float32), array([ 0.0166364 ,  0.98336363], dtype=float32), array([  9.99961138e-01,   3.88448861e-05], dtype=float32), array([  9.99980211e-01,   1.98258767e-05], dtype=float32), array([ 0.84958726,  0.15041277], dtype=float32), array([  9.99469578e-01,   5.30452642e-04], dtype=float32), array([  9.99907732e-01,   9.22226100e-05], dtype=float32), array([  9.99968529e-01,   3.14640856e-05], dtype=float32), array([ 0.91769451,  0.08230553], dtype=float32), array([ 0.61804193,  0.3819581 ], dtype=float32), array([ 0.01473562,  0.98526436], dtype=float32), array([  9.99910355e-01,   8.96546262e-05], dtype=float32), array([  1.98048274e-05,   9.99980211e-01], dtype=float32), array([  9.99952316e-01,   4.76435562e-05], dtype=float32), array([  2.63770053e-05,   9.99973655e-01], dtype=float32), array([  2.93494122e-05,   9.99970675e-01], dtype=float32), array([  9.99966145e-01,   3.38797981e-05], dtype=float32), array([ 0.99684036,  0.00315969], dtype=float32), array([ 0.01311749,  0.98688251], dtype=float32), array([  9.99958515e-01,   4.14377719e-05], dtype=float32), array([  9.99966860e-01,   3.31909578e-05], dtype=float32), array([  2.32643361e-05,   9.99976754e-01], dtype=float32), array([  9.99983907e-01,   1.60593754e-05], dtype=float32), array([  9.99923468e-01,   7.64918732e-05], dtype=float32), array([ 0.0011173 ,  0.99888271], dtype=float32), array([  9.99650002e-01,   3.50023416e-04], dtype=float32), array([  7.42093689e-05,   9.99925733e-01], dtype=float32), array([  9.99908566e-01,   9.14327684e-05], dtype=float32), array([ 0.99761581,  0.00238426], dtype=float32), array([ 0.00811667,  0.99188328], dtype=float32), array([ 0.99560189,  0.00439812], dtype=float32), array([  9.99981642e-01,   1.83119919e-05], dtype=float32), array([ 0.92496699,  0.07503301], dtype=float32), array([ 0.99221498,  0.00778507], dtype=float32), array([  8.52195837e-04,   9.99147773e-01], dtype=float32), array([  9.99981642e-01,   1.83396405e-05], dtype=float32), array([  9.99835253e-01,   1.64734636e-04], dtype=float32), array([  9.99987960e-01,   1.20455506e-05], dtype=float32), array([  9.99984860e-01,   1.50913502e-05], dtype=float32), array([  9.99411225e-01,   5.88775729e-04], dtype=float32), array([ 0.99390996,  0.00608998], dtype=float32), array([  9.99896407e-01,   1.03624719e-04], dtype=float32), array([ 0.06200935,  0.93799067], dtype=float32), array([  9.99784887e-01,   2.15149950e-04], dtype=float32), array([  9.99487162e-01,   5.12867176e-04], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# print(predictions[:test_length])\n",
    "print(predictions[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('csvtest4.csv', 'w') as csvfile:\n",
    "    csvfile.write(\",\".join(['id', 'realDonaldTrump', 'HillaryClinton'])+\"\\n\")\n",
    "    data = []\n",
    "    for i in range(test_length):\n",
    "        csvfile.write(\",\".join([str(i), str(predictions[i][1]), str(predictions[i][0])])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
