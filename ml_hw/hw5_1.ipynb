{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_input shape is: ', (4743, 2))\n",
      "('test_input shape is: ', (1701, 2))\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "train_input = pd.read_csv('train.csv')\n",
    "test_input = pd.read_csv('test.csv')\n",
    "print(\"train_input shape is: \", np.shape(train_input))\n",
    "print(\"test_input shape is: \", np.shape(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "def label_tweet(input_set):\n",
    "    handle = input_set['handle']\n",
    "    # put it into an array named label, where 0 represents HillaryClinton, 1 represents readDonaldTrump\n",
    "    label = []\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == \"HillaryClinton\":\n",
    "            label.append(0)\n",
    "        if handle[i] == \"realDonaldTrump\":\n",
    "            label.append(1)\n",
    "    label = np.asarray(label)\n",
    "    return label\n",
    "\n",
    "train_label = label_tweet(train_input)\n",
    "print(train_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4743,)\n",
      "[ 'The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG'\n",
      " 'Last night, Donald Trump said not paying taxes was \"smart.\" You know what I call it? Unpatriotic. https://t.co/t0xmBfj7zF'\n",
      " \"If we stand together, there's nothing we can't do. \\n\\nMake sure you're ready to vote: https://t.co/tTgeqxNqYm https://t.co/Q3Ymbb7UNy\"\n",
      " \"Both candidates were asked about how they'd confront racial injustice. Only one had a real answer. https://t.co/sjnEokckis\"\n",
      " 'Join me for a 3pm rally - tomorrow at the Mid-America Center in Council Bluffs, Iowa! Tickets:\\xe2\\x80\\xa6 https://t.co/dfzsbICiXc']\n",
      "(1701,)\n"
     ]
    }
   ],
   "source": [
    "train_corpus = train_input['tweet'].as_matrix()\n",
    "print(np.shape(train_corpus))\n",
    "print(train_corpus[:5])\n",
    "test_corpus = test_input['tweet'].as_matrix()\n",
    "print(np.shape(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn', u'https']\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 'https' seems useless, so I add it to stop_words\n",
    "stop_words.append(u'https')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[u'question election put plans action make life better', u'last night donald trump said paying taxes smart know call unpatriotic', u\"stand together 's nothing ca n't make sure 're ready vote\", u\"candidates asked 'd confront racial injustice one real answer\", u'join 3pm rally tomorrow mid-america center council bluffs iowa tickets']\n",
      "1701\n",
      "[u\"could n't proud hillaryclinton vision command last night 's debate showed 's ready next potus\", u\"election important sit go make sure 're registered nationalvoterregistrationday -h\", u'government people join movement today', u\"national voterregistrationday make sure 're registered vote makeamericagreatagain\\u2026\", u'great afternoon little havana hispanic community leaders thank support imwithyou']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenization(text):\n",
    "    tokens=[]\n",
    "    for word in nltk.word_tokenize(text.decode('utf-8')):\n",
    "        # skip all the websites, punctuations, pure digits\n",
    "        if not re.match('[//]', word) and re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "# Tokenize training set\n",
    "train_corpus_tokenized = []\n",
    "for i in train_corpus:\n",
    "    train_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "    \n",
    "print(len(train_corpus_tokenized))\n",
    "print(train_corpus_tokenized[:5])\n",
    "\n",
    "# Tokenize testing set\n",
    "test_corpus_tokenized = []\n",
    "for i in test_corpus:\n",
    "    test_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "print(len(test_corpus_tokenized))\n",
    "print(test_corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[['question', 'election', 'put', 'plans', 'action', 'make', 'life', 'better'], ['last', 'night', 'donald', 'trump', 'said', 'paying', 'taxes', 'smart', 'know', 'call', 'unpatriotic'], ['stand', 'together', \"'s\", 'nothing', 'ca', \"n't\", 'make', 'sure', \"'re\", 'ready', 'vote'], ['candidates', 'asked', \"'d\", 'confront', 'racial', 'injustice', 'one', 'real', 'answer'], ['join', '3pm', 'rally', 'tomorrow', 'mid-america', 'center', 'council', 'bluffs', 'iowa', 'tickets']]\n",
      "1701\n",
      "[['could', \"n't\", 'proud', 'hillaryclinton', 'vision', 'command', 'last', 'night', \"'s\", 'debate', 'showed', \"'s\", 'ready', 'next', 'potus'], ['election', 'important', 'sit', 'go', 'make', 'sure', \"'re\", 'registered', 'nationalvoterregistrationday', '-h'], ['government', 'people', 'join', 'movement', 'today'], ['national', 'voterregistrationday', 'make', 'sure', \"'re\", 'registered', 'vote', 'makeamericagreatagain\\xe2\\x80\\xa6'], ['great', 'afternoon', 'little', 'havana', 'hispanic', 'community', 'leaders', 'thank', 'support', 'imwithyou']]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_word = []\n",
    "for i in range(len(train_corpus_tokenized)):\n",
    "    train_tokenized_word.append(tf.compat.as_str(train_corpus_tokenized[i]).split())\n",
    "print(len(train_tokenized_word))\n",
    "print(train_tokenized_word[:5])\n",
    "\n",
    "vocabulary_size = 10000\n",
    "\n",
    "test_tokenized_word = []\n",
    "for i in range(len(test_corpus_tokenized)):\n",
    "    test_tokenized_word.append(tf.compat.as_str(test_corpus_tokenized[i]).split())\n",
    "print(len(test_tokenized_word))\n",
    "print(test_tokenized_word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8507\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "for i in range(len(train_tokenized_word)):\n",
    "    for word in train_tokenized_word[i]:\n",
    "        cnt[word] += 1\n",
    "\n",
    "print(len(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(cnt, words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(cnt.most_common(n_words - 1))\n",
    "#     print count[:20]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    data_all = []\n",
    "    unk_count = 0\n",
    "    for i in range(len(words)):\n",
    "        inner_data = []\n",
    "        for word in words[i]:\n",
    "            index = dictionary.get(word, 0)\n",
    "            if index == 0:  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            inner_data.append(index)\n",
    "            data_all.append(index)\n",
    "        data.append(inner_data)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, data_all, count, dictionary, reversed_dictionary\n",
    "\n",
    "train_x, data_all, count, dictionary, reverse_dictionary = build_dataset(cnt, train_tokenized_word, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n",
      "[[[92, 6, 123, 56, 1097, 0, 44, 83, 2, 101, 1224, 2, 200, 160, 32], [0, 0]], [[108, 271, 2004, 50, 12, 148, 76, 616, 1765, 81], [0, 0]], [[1276, 9, 43, 212, 27], [0, 0]], [[118, 0, 12, 148, 76, 616, 22, 0], [0, 0]], [[5, 933, 244, 0, 2910, 606, 517, 4, 66, 168], [0, 0]]]\n",
      "1701\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for sentence in test_tokenized_word:\n",
    "    cur = []\n",
    "    for word in sentence:\n",
    "        if(word in dictionary):\n",
    "            cur.append(dictionary[word])\n",
    "        else:\n",
    "            cur.append(0)\n",
    "    test.append([cur,[0, 0]])\n",
    "\n",
    "print(len(test))\n",
    "print(test[:5])\n",
    "test_length = len(test)\n",
    "print(test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3794\n",
      "[[[7081, 316, 751, 686, 7795, 497, 1033, 1687, 5289, 182, 6696, 316, 2912, 106, 497, 1033, 6139, 500, 3575, 1261, 4295, 4510], [0, 1]], [[288, 1446, 34, 1283, 1225, 493, 1961, 9, 1771, 2, 2521], [0, 1]], [[7, 1, 37, 565, 73, 373, 321, 2, 7391, 2470, 514, 108, 6, 24], [0, 1]], [[344, 174, 2131, 40, 1602, 36, 362, 2290, 71, 1619, 613, 1896, 334, 3229, 535], [1, 0]], [[5, 579, 1038, 218, 179, 28, 3, 6884, 334, 132, 382], [1, 0]]]\n",
      "949\n",
      "[[[5203, 66, 3815, 50, 277, 150, 283, 3140], [1, 0]], [[3074, 105, 44, 202, 38, 3074, 11], [0, 1]], [[6734, 17, 118, 862, 2770, 735, 354, 7798, 143, 25, 1032, 6, 71], [1, 0]], [[1, 5271, 962, 1728, 689, 379, 1077, 67, 192, 985, 1026], [0, 1]], [[500, 6537, 2834, 2976, 1897, 4904, 1261, 592, 7115, 4004, 5268], [0, 1]]]\n",
      "8508\n"
     ]
    }
   ],
   "source": [
    "train_all = [[train_x[i], [train_label[i], 1-train_label[i]]] for i in range(0, len(train_x))]\n",
    "# train_all = [[train_x[i], [train_label[i]]] for i in range(0, len(train_x))]\n",
    "\n",
    "r_index = list(range(len(train_all)))\n",
    "random.shuffle(r_index)\n",
    "train = [train_all[i] for i in r_index[:int(len(r_index)*0.8)]]\n",
    "valid = [train_all[i] for i in r_index[int(len(r_index)*0.8):]]\n",
    "\n",
    "print(len(train))\n",
    "print(train[:5])\n",
    "print(len(valid))\n",
    "print(valid[:5])\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.df)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            print(\"SimpleDataIterator epoch : \", self.epochs)\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input sequences : ', [[[1145, 127, 4974, 109, 2023, 222, 35, 940, 45, 12, 10, 5], [1, 0]], [[93, 823, 3735, 15, 2, 7165, 3075, 47, 7299, 979, 144, 2166, 19, 22, 458, 2488], [1, 0]], [[4], [0, 1]], [[8111, 316, 949, 1178, 3374, 1157, 1739, 1362, 1123], [0, 1]], [[2063, 1, 2, 5181, 959, 140, 14], [1, 0]]])\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(valid)\n",
    "d = data.next_batch(500)\n",
    "print('Input sequences : ', d[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        max_len = 0\n",
    "        for row in res:\n",
    "            if len(row[0]) > max_len:\n",
    "                max_len = len(row[0])\n",
    "        seqlen = np.array([max_len for i in range(len(res))])\n",
    "        ret = []\n",
    "        label = []\n",
    "        for row in res:\n",
    "            ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "            label.append(row[1][0])\n",
    "        x = np.array(ret)\n",
    "        y = np.array(label)\n",
    "\n",
    "        return x, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input sequences\\n', array([[ 175, 8339,    1, 8315, 3315, 7494,   13, 3938, 6364,  110,   29,\n",
      "          95,  432],\n",
      "       [  60,  125,  434,  268,  438,   90,  370,    1, 1079,    0,    0,\n",
      "           0,    0],\n",
      "       [ 904,   39, 8277,  698,    3, 2730,   39, 2386, 2087,  681, 3689,\n",
      "           0,    0]]))\n",
      "(3, 13)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "[13 13 13]\n"
     ]
    }
   ],
   "source": [
    "data = PaddedDataIterator(train)\n",
    "d = data.next_batch(3)\n",
    "print('Input sequences\\n', d[0])\n",
    "print(d[0].shape)\n",
    "print(d[1])\n",
    "print(d[1].shape)\n",
    "print(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n",
      "256\n",
      "6\n",
      "0\n",
      "256\n",
      "19\n",
      "((256, 19), ', ', (256,), ', ', (256,))\n",
      "1\n",
      "256\n",
      "18\n",
      "((256, 18), ', ', (256,), ', ', (256,))\n",
      "2\n",
      "256\n",
      "23\n",
      "((256, 23), ', ', (256,), ', ', (256,))\n",
      "3\n",
      "256\n",
      "20\n",
      "((256, 20), ', ', (256,), ', ', (256,))\n",
      "4\n",
      "256\n",
      "20\n",
      "((256, 20), ', ', (256,), ', ', (256,))\n",
      "5\n",
      "256\n",
      "18\n",
      "((256, 18), ', ', (256,), ', ', (256,))\n",
      "6\n",
      "256\n",
      "19\n",
      "((256, 19), ', ', (256,), ', ', (256,))\n",
      "=====\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def align(data):\n",
    "    print(len(data))\n",
    "    max_len = 0\n",
    "    for row in data:\n",
    "        if len(row[0]) > max_len:\n",
    "            max_len = len(row[0])\n",
    "    print(max_len)\n",
    "    ret = []\n",
    "    label = []\n",
    "    for row in data:\n",
    "        ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "        label.append(row[1][0])\n",
    "    x = np.array(ret)\n",
    "    y = np.array(label)\n",
    "    seq_len = np.array([max_len for i in data])\n",
    "    \n",
    "    return x, y, seq_len\n",
    "\n",
    "# def xx():\n",
    "#     max_len = 0\n",
    "#     for row in res:\n",
    "#         if len(row[0]) > max_len:\n",
    "#             max_len = len(row[0])\n",
    "#     seqlen = np.array([max_len for i in range(len(res))])\n",
    "#     ret = []\n",
    "#     label = []\n",
    "#     for row in res:\n",
    "#         ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "#         label.append(row[1][0])\n",
    "#     x = np.array(ret)\n",
    "#     y = np.array(label)\n",
    "\n",
    "# test_length = 1701\n",
    "print(test_length)\n",
    "print(batch_size)\n",
    "print(test_length/batch_size)\n",
    "\n",
    "test_list = []\n",
    "test_addlen = test\n",
    "test_addlen.extend(test[0:batch_size])\n",
    "for i in range(test_length/batch_size+1):\n",
    "    print(i)\n",
    "    x, y, seq_len = align(test[i*batch_size:(i+1)*batch_size])\n",
    "    print(x.shape, \", \", y.shape, \", \", seq_len.shape)\n",
    "#     print(y.shape)\n",
    "#     print(seq_len.shape)\n",
    "    test_list.append([x, y, seq_len])\n",
    "\n",
    "print(\"=====\")\n",
    "print(len(test_list[0]))\n",
    "    \n",
    "\n",
    "# test_align, max_len = align(test)\n",
    "# print(len(test_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = len(dictionary),\n",
    "    state_size = 64,\n",
    "    batch_size = 256,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "#     keep_prob = tf.constant(1.0)\n",
    "#     print(\"====\",x.shape)\n",
    "#     print(\"====\",y.shape)\n",
    "#     print(\"====\",seqlen.shape)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "#     rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    \"\"\"\n",
    "    Obtain the last relevant output. The best approach in the future will be to use:\n",
    "\n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    which is the Tensorflow equivalent of numpy's rnn_outputs[range(30), seqlen-1, :], but the\n",
    "    gradient for this op has not been implemented as of this writing.\n",
    "\n",
    "    The below solution works, but throws a UserWarning re: the gradient.\n",
    "    \"\"\"\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "#     print(correct.shape)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "#     print(accuracy.shape)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "#         'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 256, num_epochs = 30, iterator = PaddedDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        tv = iterator(valid)\n",
    "#         te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, tv_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "#             print(\"----\",batch[0].shape)\n",
    "#             print(\"----\",batch[1].shape)\n",
    "#             print(\"----\",batch[2].shape)\n",
    "#             dropout_parameter=np.array(0.6)\n",
    "#             print(\"----\",dropout_parameter.shape)\n",
    "#             feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: dropout_parameter}\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                tv_epoch = tv.epochs\n",
    "                while tv.epochs == tv_epoch:\n",
    "                    step += 1\n",
    "                    batch = tv.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                tv_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- tv:\", tv_losses[-1])\n",
    "            \n",
    "        predictions = []\n",
    "        for te in test_list:\n",
    "            feed = {g['x']: te[0], g['y']: te[1], g['seqlen']: te[2]}\n",
    "            preds_, _ = sess.run([g['preds'], g['ts']], feed_dict=feed)\n",
    "            print(len(preds_))\n",
    "            predictions.extend(preds_)\n",
    "\n",
    "    return tr_losses, tv_losses, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy after epoch', 1, ' - tr:', 0.50312500000000004, '- tv:', 0.5517578125)\n",
      "('Accuracy after epoch', 2, ' - tr:', 0.537109375, '- tv:', 0.52864583333333337)\n",
      "('Accuracy after epoch', 3, ' - tr:', 0.5421316964285714, '- tv:', 0.59765625)\n",
      "('Accuracy after epoch', 4, ' - tr:', 0.66015625, '- tv:', 0.77604166666666663)\n",
      "('Accuracy after epoch', 5, ' - tr:', 0.8797433035714286, '- tv:', 0.85286458333333337)\n",
      "('Accuracy after epoch', 6, ' - tr:', 0.8869977678571429, '- tv:', 0.86067708333333337)\n",
      "('Accuracy after epoch', 7, ' - tr:', 0.935546875, '- tv:', 0.87239583333333337)\n",
      "('Accuracy after epoch', 8, ' - tr:', 0.9405691964285714, '- tv:', 0.90364583333333337)\n",
      "('Accuracy after epoch', 9, ' - tr:', 0.9458705357142857, '- tv:', 0.88932291666666663)\n",
      "('Accuracy after epoch', 10, ' - tr:', 0.9497767857142857, '- tv:', 0.89973958333333337)\n",
      "('Accuracy after epoch', 11, ' - tr:', 0.9506138392857143, '- tv:', 0.91536458333333337)\n",
      "('Accuracy after epoch', 12, ' - tr:', 0.9542410714285714, '- tv:', 0.89192708333333337)\n",
      "('Accuracy after epoch', 13, ' - tr:', 0.9606584821428571, '- tv:', 0.90625)\n",
      "('Accuracy after epoch', 14, ' - tr:', 0.9693080357142857, '- tv:', 0.89322916666666663)\n",
      "('Accuracy after epoch', 15, ' - tr:', 0.974609375, '- tv:', 0.9140625)\n",
      "('Accuracy after epoch', 16, ' - tr:', 0.9796316964285714, '- tv:', 0.91015625)\n",
      "('Accuracy after epoch', 17, ' - tr:', 0.9840959821428571, '- tv:', 0.90104166666666663)\n",
      "('Accuracy after epoch', 18, ' - tr:', 0.98828125, '- tv:', 0.91145833333333337)\n",
      "('Accuracy after epoch', 19, ' - tr:', 0.9893973214285714, '- tv:', 0.92317708333333337)\n",
      "('Accuracy after epoch', 20, ' - tr:', 0.9893973214285714, '- tv:', 0.90755208333333337)\n",
      "('Accuracy after epoch', 21, ' - tr:', 0.9924665178571429, '- tv:', 0.91015625)\n",
      "('Accuracy after epoch', 22, ' - tr:', 0.9916294642857143, '- tv:', 0.91145833333333337)\n",
      "('Accuracy after epoch', 23, ' - tr:', 0.994140625, '- tv:', 0.90755208333333337)\n",
      "('Accuracy after epoch', 24, ' - tr:', 0.9958147321428571, '- tv:', 0.90625)\n",
      "('Accuracy after epoch', 25, ' - tr:', 0.9949776785714286, '- tv:', 0.91276041666666663)\n",
      "('Accuracy after epoch', 26, ' - tr:', 0.9972098214285714, '- tv:', 0.90104166666666663)\n",
      "('Accuracy after epoch', 27, ' - tr:', 0.9969308035714286, '- tv:', 0.9140625)\n",
      "('Accuracy after epoch', 28, ' - tr:', 0.998046875, '- tv:', 0.89973958333333337)\n",
      "('Accuracy after epoch', 29, ' - tr:', 0.9972098214285714, '- tv:', 0.89583333333333337)\n",
      "('Accuracy after epoch', 30, ' - tr:', 0.9974888392857143, '- tv:', 0.90755208333333337)\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "1792\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, tv_losses, predictions = train_graph(g)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.99508613,  0.00491382], dtype=float32), array([ 0.99898177,  0.00101818], dtype=float32), array([  8.36639025e-04,   9.99163389e-01], dtype=float32), array([ 0.99675131,  0.00324868], dtype=float32), array([ 0.00122388,  0.99877614], dtype=float32), array([ 0.9981522 ,  0.00184773], dtype=float32), array([ 0.99545527,  0.00454476], dtype=float32), array([ 0.99864632,  0.00135366], dtype=float32), array([ 0.99890935,  0.0010906 ], dtype=float32), array([ 0.036767  ,  0.96323293], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# print(predictions[:test_length])\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = file('csvtest1.csv', 'wb')\n",
    "writer = csv.writer(csvfile)\n",
    "writer.writerow(['id', 'realDonaldTrump', 'HillaryClinton'])\n",
    "data = []\n",
    "for i in range(test_length):\n",
    "    data.append((i, predictions[i][0], predictions[i][1]))\n",
    "\n",
    "# data = [\n",
    "#   ('1', 'http://www.xiaoheiseo.com/', '小黑'),\n",
    "#   ('2', 'http://www.baidu.com/', '百度'),\n",
    "#   ('3', 'http://www.jd.com/', '京东')\n",
    "# ]\n",
    "writer.writerows(data)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
