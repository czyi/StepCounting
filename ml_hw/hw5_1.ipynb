{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape is:  (4743, 2)\n",
      "test_input shape is:  (1701, 2)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "train_input = pd.read_csv('train.csv')\n",
    "test_input = pd.read_csv('test.csv')\n",
    "print(\"train_input shape is: \", np.shape(train_input))\n",
    "print(\"test_input shape is: \", np.shape(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "def label_tweet(input_set):\n",
    "    handle = input_set['handle']\n",
    "    # put it into an array named label, where 0 represents HillaryClinton, 1 represents readDonaldTrump\n",
    "    label = []\n",
    "    for i in range(len(handle)):\n",
    "        if handle[i] == \"HillaryClinton\":\n",
    "            label.append(0)\n",
    "        if handle[i] == \"realDonaldTrump\":\n",
    "            label.append(1)\n",
    "    label = np.asarray(label)\n",
    "    return label\n",
    "\n",
    "train_label = label_tweet(train_input)\n",
    "print(train_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4743,)\n",
      "[ 'The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG'\n",
      " 'Last night, Donald Trump said not paying taxes was \"smart.\" You know what I call it? Unpatriotic. https://t.co/t0xmBfj7zF'\n",
      " \"If we stand together, there's nothing we can't do. \\n\\nMake sure you're ready to vote: https://t.co/tTgeqxNqYm https://t.co/Q3Ymbb7UNy\"\n",
      " \"Both candidates were asked about how they'd confront racial injustice. Only one had a real answer. https://t.co/sjnEokckis\"\n",
      " 'Join me for a 3pm rally - tomorrow at the Mid-America Center in Council Bluffs, Iowa! Tickets:… https://t.co/dfzsbICiXc']\n",
      "(1701,)\n"
     ]
    }
   ],
   "source": [
    "train_corpus = train_input['tweet'].as_matrix()\n",
    "print(np.shape(train_corpus))\n",
    "print(train_corpus[:5])\n",
    "test_corpus = test_input['tweet'].as_matrix()\n",
    "print(np.shape(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'https']\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 'https' seems useless, so I add it to stop_words\n",
    "stop_words.append(u'https')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "['T h e   q u e s t i o n   i n   t h i s   e l e c t i o n :   W h o   c a n   p u t   t h e   p l a n s   i n t o   a c t i o n   t h a t   w i l l   m a k e   y o u r   l i f e   b e t t e r ?   h t t p s : / / t . c o / X r e E Y 9 O i c G', 'L a s t   n i g h t ,   D o n a l d   T r u m p   s a i d   n o t   p a y i n g   t a x e s   w a s   \" s m a r t . \"   Y o u   k n o w   w h a t   I   c a l l   i t ?   U n p a t r i o t i c .   h t t p s : / / t . c o / t 0 x m B f j 7 z F', \"I f   w e   s t a n d   t o g e t h e r ,   t h e r e ' s   n o t h i n g   w e   c a n ' t   d o .   \\n \\n M a k e   s u r e   y o u ' r e   r e a d y   t o   v o t e :   h t t p s : / / t . c o / t T g e q x N q Y m   h t t p s : / / t . c o / Q 3 Y m b b 7 U N y\", \"B o t h   c a n d i d a t e s   w e r e   a s k e d   a b o u t   h o w   t h e y ' d   c o n f r o n t   r a c i a l   i n j u s t i c e .   O n l y   o n e   h a d   a   r e a l   a n s w e r .   h t t p s : / / t . c o / s j n E o k c k i s\", 'J o i n   m e   f o r   a   3 p m   r a l l y   -   t o m o r r o w   a t   t h e   M i d - A m e r i c a   C e n t e r   i n   C o u n c i l   B l u f f s ,   I o w a !   T i c k e t s : …   h t t p s : / / t . c o / d f z s b I C i X c']\n",
      "1701\n",
      "[\"C o u l d n ' t   b e   m o r e   p r o u d   o f   @ H i l l a r y C l i n t o n .   H e r   v i s i o n   a n d   c o m m a n d   d u r i n g   l a s t   n i g h t ' s   d e b a t e   s h o w e d   t h a t   s h e ' s   r e a d y   t o   b e   o u r   n e x t   @ P O T U S .\", \"T h i s   e l e c t i o n   i s   t o o   i m p o r t a n t   t o   s i t   o u t .   G o   t o   h t t p s : / / t . c o / t T g e q x N q Y m   a n d   m a k e   s u r e   y o u ' r e   r e g i s t e r e d .   # N a t i o n a l V o t e r R e g i s t r a t i o n D a y   - H\", 'O n c e   a g a i n ,   w e   w i l l   h a v e   a   g o v e r n m e n t   o f ,   b y   a n d   f o r   t h e   p e o p l e .   J o i n   t h e   M O V E M E N T   t o d a y !   h t t p s : / / t . c o / l W j Y D b P H a v   h t t p s : / / t . c o / u Y w J r t Z k A e', \"O n   N a t i o n a l   # V o t e r R e g i s t r a t i o n D a y ,   m a k e   s u r e   y o u ' r e   r e g i s t e r e d   t o   v o t e   s o   w e   c a n   # M a k e A m e r i c a G r e a t A g a i n …   h t t p s : / / t . c o / 0 w i b 6 U E Z O N\", 'G r e a t   a f t e r n o o n   i n   L i t t l e   H a v a n a   w i t h   H i s p a n i c   c o m m u n i t y   l e a d e r s .   T h a n k   y o u   f o r   y o u r   s u p p o r t !   # I m W i t h Y o u   h t t p s : / / t . c o / v x W Z 2 t y J T F']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenization(text):\n",
    "    return [i for i in text]\n",
    "#     tokens=[]\n",
    "# #     for word in nltk.word_tokenize(text.decode('utf-8')):\n",
    "#     for word in nltk.word_tokenize(text):\n",
    "#         # skip all the websites, punctuations, pure digits\n",
    "#         if not re.match('[//]', word) and re.search('[a-zA-Z]', word) and word.lower() not in stop_words:\n",
    "#             tokens.append(word.lower())\n",
    "#     return tokens\n",
    "\n",
    "# Tokenize training set\n",
    "train_corpus_tokenized = []\n",
    "for i in train_corpus:\n",
    "    train_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "    \n",
    "print(len(train_corpus_tokenized))\n",
    "print(train_corpus_tokenized[:5])\n",
    "\n",
    "# Tokenize testing set\n",
    "test_corpus_tokenized = []\n",
    "for i in test_corpus:\n",
    "    test_corpus_tokenized.append(' '.join(tokenization(i)))\n",
    "\n",
    "print(len(test_corpus_tokenized))\n",
    "print(test_corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743\n",
      "[['T', 'h', 'e', 'q', 'u', 'e', 's', 't', 'i', 'o', 'n', 'i', 'n', 't', 'h', 'i', 's', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', ':', 'W', 'h', 'o', 'c', 'a', 'n', 'p', 'u', 't', 't', 'h', 'e', 'p', 'l', 'a', 'n', 's', 'i', 'n', 't', 'o', 'a', 'c', 't', 'i', 'o', 'n', 't', 'h', 'a', 't', 'w', 'i', 'l', 'l', 'm', 'a', 'k', 'e', 'y', 'o', 'u', 'r', 'l', 'i', 'f', 'e', 'b', 'e', 't', 't', 'e', 'r', '?', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'X', 'r', 'e', 'E', 'Y', '9', 'O', 'i', 'c', 'G'], ['L', 'a', 's', 't', 'n', 'i', 'g', 'h', 't', ',', 'D', 'o', 'n', 'a', 'l', 'd', 'T', 'r', 'u', 'm', 'p', 's', 'a', 'i', 'd', 'n', 'o', 't', 'p', 'a', 'y', 'i', 'n', 'g', 't', 'a', 'x', 'e', 's', 'w', 'a', 's', '\"', 's', 'm', 'a', 'r', 't', '.', '\"', 'Y', 'o', 'u', 'k', 'n', 'o', 'w', 'w', 'h', 'a', 't', 'I', 'c', 'a', 'l', 'l', 'i', 't', '?', 'U', 'n', 'p', 'a', 't', 'r', 'i', 'o', 't', 'i', 'c', '.', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 't', '0', 'x', 'm', 'B', 'f', 'j', '7', 'z', 'F'], ['I', 'f', 'w', 'e', 's', 't', 'a', 'n', 'd', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ',', 't', 'h', 'e', 'r', 'e', \"'\", 's', 'n', 'o', 't', 'h', 'i', 'n', 'g', 'w', 'e', 'c', 'a', 'n', \"'\", 't', 'd', 'o', '.', 'M', 'a', 'k', 'e', 's', 'u', 'r', 'e', 'y', 'o', 'u', \"'\", 'r', 'e', 'r', 'e', 'a', 'd', 'y', 't', 'o', 'v', 'o', 't', 'e', ':', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 't', 'T', 'g', 'e', 'q', 'x', 'N', 'q', 'Y', 'm', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'Q', '3', 'Y', 'm', 'b', 'b', '7', 'U', 'N', 'y'], ['B', 'o', 't', 'h', 'c', 'a', 'n', 'd', 'i', 'd', 'a', 't', 'e', 's', 'w', 'e', 'r', 'e', 'a', 's', 'k', 'e', 'd', 'a', 'b', 'o', 'u', 't', 'h', 'o', 'w', 't', 'h', 'e', 'y', \"'\", 'd', 'c', 'o', 'n', 'f', 'r', 'o', 'n', 't', 'r', 'a', 'c', 'i', 'a', 'l', 'i', 'n', 'j', 'u', 's', 't', 'i', 'c', 'e', '.', 'O', 'n', 'l', 'y', 'o', 'n', 'e', 'h', 'a', 'd', 'a', 'r', 'e', 'a', 'l', 'a', 'n', 's', 'w', 'e', 'r', '.', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 's', 'j', 'n', 'E', 'o', 'k', 'c', 'k', 'i', 's'], ['J', 'o', 'i', 'n', 'm', 'e', 'f', 'o', 'r', 'a', '3', 'p', 'm', 'r', 'a', 'l', 'l', 'y', '-', 't', 'o', 'm', 'o', 'r', 'r', 'o', 'w', 'a', 't', 't', 'h', 'e', 'M', 'i', 'd', '-', 'A', 'm', 'e', 'r', 'i', 'c', 'a', 'C', 'e', 'n', 't', 'e', 'r', 'i', 'n', 'C', 'o', 'u', 'n', 'c', 'i', 'l', 'B', 'l', 'u', 'f', 'f', 's', ',', 'I', 'o', 'w', 'a', '!', 'T', 'i', 'c', 'k', 'e', 't', 's', ':', '…', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'd', 'f', 'z', 's', 'b', 'I', 'C', 'i', 'X', 'c']]\n",
      "1701\n",
      "[['C', 'o', 'u', 'l', 'd', 'n', \"'\", 't', 'b', 'e', 'm', 'o', 'r', 'e', 'p', 'r', 'o', 'u', 'd', 'o', 'f', '@', 'H', 'i', 'l', 'l', 'a', 'r', 'y', 'C', 'l', 'i', 'n', 't', 'o', 'n', '.', 'H', 'e', 'r', 'v', 'i', 's', 'i', 'o', 'n', 'a', 'n', 'd', 'c', 'o', 'm', 'm', 'a', 'n', 'd', 'd', 'u', 'r', 'i', 'n', 'g', 'l', 'a', 's', 't', 'n', 'i', 'g', 'h', 't', \"'\", 's', 'd', 'e', 'b', 'a', 't', 'e', 's', 'h', 'o', 'w', 'e', 'd', 't', 'h', 'a', 't', 's', 'h', 'e', \"'\", 's', 'r', 'e', 'a', 'd', 'y', 't', 'o', 'b', 'e', 'o', 'u', 'r', 'n', 'e', 'x', 't', '@', 'P', 'O', 'T', 'U', 'S', '.'], ['T', 'h', 'i', 's', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', 'i', 's', 't', 'o', 'o', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', 't', 'o', 's', 'i', 't', 'o', 'u', 't', '.', 'G', 'o', 't', 'o', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 't', 'T', 'g', 'e', 'q', 'x', 'N', 'q', 'Y', 'm', 'a', 'n', 'd', 'm', 'a', 'k', 'e', 's', 'u', 'r', 'e', 'y', 'o', 'u', \"'\", 'r', 'e', 'r', 'e', 'g', 'i', 's', 't', 'e', 'r', 'e', 'd', '.', '#', 'N', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'V', 'o', 't', 'e', 'r', 'R', 'e', 'g', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', 'D', 'a', 'y', '-', 'H'], ['O', 'n', 'c', 'e', 'a', 'g', 'a', 'i', 'n', ',', 'w', 'e', 'w', 'i', 'l', 'l', 'h', 'a', 'v', 'e', 'a', 'g', 'o', 'v', 'e', 'r', 'n', 'm', 'e', 'n', 't', 'o', 'f', ',', 'b', 'y', 'a', 'n', 'd', 'f', 'o', 'r', 't', 'h', 'e', 'p', 'e', 'o', 'p', 'l', 'e', '.', 'J', 'o', 'i', 'n', 't', 'h', 'e', 'M', 'O', 'V', 'E', 'M', 'E', 'N', 'T', 't', 'o', 'd', 'a', 'y', '!', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'l', 'W', 'j', 'Y', 'D', 'b', 'P', 'H', 'a', 'v', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'u', 'Y', 'w', 'J', 'r', 't', 'Z', 'k', 'A', 'e'], ['O', 'n', 'N', 'a', 't', 'i', 'o', 'n', 'a', 'l', '#', 'V', 'o', 't', 'e', 'r', 'R', 'e', 'g', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', 'D', 'a', 'y', ',', 'm', 'a', 'k', 'e', 's', 'u', 'r', 'e', 'y', 'o', 'u', \"'\", 'r', 'e', 'r', 'e', 'g', 'i', 's', 't', 'e', 'r', 'e', 'd', 't', 'o', 'v', 'o', 't', 'e', 's', 'o', 'w', 'e', 'c', 'a', 'n', '#', 'M', 'a', 'k', 'e', 'A', 'm', 'e', 'r', 'i', 'c', 'a', 'G', 'r', 'e', 'a', 't', 'A', 'g', 'a', 'i', 'n', '…', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', '0', 'w', 'i', 'b', '6', 'U', 'E', 'Z', 'O', 'N'], ['G', 'r', 'e', 'a', 't', 'a', 'f', 't', 'e', 'r', 'n', 'o', 'o', 'n', 'i', 'n', 'L', 'i', 't', 't', 'l', 'e', 'H', 'a', 'v', 'a', 'n', 'a', 'w', 'i', 't', 'h', 'H', 'i', 's', 'p', 'a', 'n', 'i', 'c', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 't', 'y', 'l', 'e', 'a', 'd', 'e', 'r', 's', '.', 'T', 'h', 'a', 'n', 'k', 'y', 'o', 'u', 'f', 'o', 'r', 'y', 'o', 'u', 'r', 's', 'u', 'p', 'p', 'o', 'r', 't', '!', '#', 'I', 'm', 'W', 'i', 't', 'h', 'Y', 'o', 'u', 'h', 't', 't', 'p', 's', ':', '/', '/', 't', '.', 'c', 'o', '/', 'v', 'x', 'W', 'Z', '2', 't', 'y', 'J', 'T', 'F']]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_word = []\n",
    "for i in range(len(train_corpus_tokenized)):\n",
    "    train_tokenized_word.append(tf.compat.as_str(train_corpus_tokenized[i]).split())\n",
    "print(len(train_tokenized_word))\n",
    "print(train_tokenized_word[:5])\n",
    "\n",
    "vocabulary_size = 10000\n",
    "\n",
    "test_tokenized_word = []\n",
    "for i in range(len(test_corpus_tokenized)):\n",
    "    test_tokenized_word.append(tf.compat.as_str(test_corpus_tokenized[i]).split())\n",
    "print(len(test_tokenized_word))\n",
    "print(test_tokenized_word[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "cnt = collections.Counter()\n",
    "for i in range(len(train_tokenized_word)):\n",
    "    for word in train_tokenized_word[i]:\n",
    "        cnt[word] += 1\n",
    "\n",
    "print(len(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(cnt, words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(cnt.most_common(n_words - 1))\n",
    "#     print count[:20]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    data_all = []\n",
    "    unk_count = 0\n",
    "    for i in range(len(words)):\n",
    "        inner_data = []\n",
    "        for word in words[i]:\n",
    "            index = dictionary.get(word, 0)\n",
    "            if index == 0:  # dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            inner_data.append(index)\n",
    "            data_all.append(index)\n",
    "        data.append(inner_data)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, data_all, count, dictionary, reversed_dictionary\n",
    "\n",
    "train_x, data_all, count, dictionary, reverse_dictionary = build_dataset(cnt, train_tokenized_word, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n",
      "[[[29, 3, 14, 10, 11, 5, 45, 2, 22, 1, 15, 3, 7, 1, 13, 7, 3, 14, 11, 3, 21, 33, 31, 6, 10, 10, 4, 7, 18, 29, 10, 6, 5, 2, 3, 5, 17, 31, 1, 7, 25, 6, 8, 6, 3, 5, 4, 5, 11, 12, 3, 15, 15, 4, 5, 11, 11, 14, 7, 6, 5, 19, 10, 4, 8, 2, 5, 6, 19, 9, 2, 45, 8, 11, 1, 22, 4, 2, 1, 8, 9, 3, 20, 1, 11, 2, 9, 4, 2, 8, 9, 1, 45, 8, 7, 1, 4, 11, 18, 2, 3, 22, 1, 3, 14, 7, 5, 1, 53, 2, 33, 42, 43, 23, 55, 32, 17], [0, 0]], [[23, 9, 6, 8, 1, 10, 1, 12, 2, 6, 3, 5, 6, 8, 2, 3, 3, 6, 15, 13, 3, 7, 2, 4, 5, 2, 2, 3, 8, 6, 2, 3, 14, 2, 17, 44, 3, 2, 3, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 2, 23, 19, 1, 64, 53, 34, 64, 60, 15, 4, 5, 11, 15, 4, 26, 1, 8, 14, 7, 1, 18, 3, 14, 45, 7, 1, 7, 1, 19, 6, 8, 2, 1, 7, 1, 11, 17, 46, 34, 4, 2, 6, 3, 5, 4, 10, 61, 3, 2, 1, 7, 40, 1, 19, 6, 8, 2, 7, 4, 2, 6, 3, 5, 36, 4, 18, 59, 31], [0, 0]], [[43, 5, 12, 1, 4, 19, 4, 6, 5, 30, 20, 1, 20, 6, 10, 10, 9, 4, 25, 1, 4, 19, 3, 25, 1, 7, 5, 15, 1, 5, 2, 3, 21, 30, 22, 18, 4, 5, 11, 21, 3, 7, 2, 9, 1, 13, 1, 3, 13, 10, 1, 17, 56, 3, 6, 5, 2, 9, 1, 37, 43, 61, 41, 37, 41, 34, 23, 2, 3, 11, 4, 18, 38, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 10, 39, 51, 60, 36, 22, 42, 31, 4, 25, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 14, 60, 20, 56, 7, 2, 73, 26, 27, 1], [0, 0]], [[43, 5, 34, 4, 2, 6, 3, 5, 4, 10, 46, 61, 3, 2, 1, 7, 40, 1, 19, 6, 8, 2, 7, 4, 2, 6, 3, 5, 36, 4, 18, 30, 15, 4, 26, 1, 8, 14, 7, 1, 18, 3, 14, 45, 7, 1, 7, 1, 19, 6, 8, 2, 1, 7, 1, 11, 2, 3, 25, 3, 2, 1, 8, 3, 20, 1, 12, 4, 5, 46, 37, 4, 26, 1, 27, 15, 1, 7, 6, 12, 4, 44, 7, 1, 4, 2, 27, 19, 4, 6, 5, 85, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 47, 20, 6, 22, 57, 55, 41, 73, 43, 34], [0, 0]], [[44, 7, 1, 4, 2, 4, 21, 2, 1, 7, 5, 3, 3, 5, 6, 5, 49, 6, 2, 2, 10, 1, 31, 4, 25, 4, 5, 4, 20, 6, 2, 9, 31, 6, 8, 13, 4, 5, 6, 12, 12, 3, 15, 15, 14, 5, 6, 2, 18, 10, 1, 4, 11, 1, 7, 8, 17, 23, 9, 4, 5, 26, 18, 3, 14, 21, 3, 7, 18, 3, 14, 7, 8, 14, 13, 13, 3, 7, 2, 38, 46, 28, 15, 39, 6, 2, 9, 60, 3, 14, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 25, 53, 39, 73, 54, 2, 18, 56, 23, 52], [0, 0]]]\n",
      "1701\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for sentence in test_tokenized_word:\n",
    "    cur = []\n",
    "    for word in sentence:\n",
    "        if(word in dictionary):\n",
    "            cur.append(dictionary[word])\n",
    "        else:\n",
    "            cur.append(0)\n",
    "    test.append([cur,[0, 0]])\n",
    "\n",
    "print(len(test))\n",
    "print(test[:5])\n",
    "test_length = len(test)\n",
    "print(test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3794\n",
      "[[[17, 33, 29, 34, 34, 13, 3, 10, 10, 51, 14, 8, 2, 9, 6, 2, 66, 71, 84, 21, 3, 7, 23, 7, 14, 15, 13, 17, 28, 5, 2, 1, 7, 1, 8, 2, 6, 5, 19, 9, 3, 20, 15, 18, 5, 14, 15, 22, 1, 7, 8, 9, 4, 25, 1, 19, 3, 5, 1, 8, 3, 21, 4, 7, 14, 13, 8, 6, 5, 12, 1, 10, 6, 19, 9, 2, 20, 1, 6, 19, 9, 2, 37, 4, 7, 12, 3, 40, 14, 22, 6, 3, 9, 4, 8, 2, 14, 7, 5, 1, 11, 5, 4, 8, 2, 18, 17, 49, 3, 25, 1, 6, 2, 38], [1, 0]], [[39, 1, 9, 4, 25, 1, 2, 9, 1, 15, 3, 8, 2, 13, 7, 3, 11, 14, 12, 2, 6, 25, 1, 20, 3, 7, 26, 1, 7, 8, 6, 5, 2, 9, 1, 20, 3, 7, 10, 11, 17, 39, 1, 51, 14, 8, 2, 5, 1, 1, 11, 2, 3, 19, 6, 25, 1, 3, 14, 7, 13, 1, 3, 13, 10, 1, 4, 12, 9, 4, 5, 12, 1, 2, 3, 8, 14, 12, 12, 1, 1, 11, 24, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 27, 43, 34, 5, 65, 19, 22, 4, 12, 9], [0, 1]], [[35, 33, 31, 27, 40, 18, 11, 1, 7, 24, 39, 9, 6, 12, 9, 6, 8, 6, 2, 33, 7, 1, 4, 10, 36, 3, 5, 4, 10, 11, 23, 7, 14, 15, 13, 76, 27, 7, 1, 18, 3, 14, 13, 10, 4, 5, 5, 6, 5, 19, 3, 5, 19, 1, 2, 2, 6, 5, 19, 7, 6, 11, 3, 21, 29, 3, 15, 15, 3, 5, 29, 3, 7, 1, 3, 7, 26, 1, 1, 13, 6, 5, 19, 6, 2, 76, 44, 1, 2, 7, 6, 11, 3, 21, 6, 2, 21, 4, 8, 2, 17], [1, 0]], [[23, 9, 4, 5, 26, 18, 3, 14, 52, 3, 7, 2, 39, 4, 18, 5, 1, 30, 28, 5, 11, 6, 4, 5, 4, 38, 46, 23, 7, 14, 15, 13, 54, 47, 48, 57, 46, 28, 34, 42, 7, 6, 15, 4, 7, 18, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 15, 36, 44, 7, 15, 37, 15, 26, 68, 23], [1, 0]], [[37, 6, 2, 2, 40, 3, 15, 5, 1, 18, 9, 4, 11, 9, 6, 8, 12, 9, 4, 5, 12, 1, 2, 3, 22, 1, 4, 2, 4, 21, 4, 6, 10, 1, 11, 13, 7, 1, 8, 6, 11, 1, 5, 2, 22, 14, 2, 9, 1, 12, 9, 3, 26, 1, 11, 10, 6, 26, 1, 4, 11, 3, 19, 17, 34, 3, 20, 9, 1, 12, 4, 10, 10, 8, 15, 1, 7, 4, 12, 6, 8, 2, 59, 22, 14, 2, 28, 4, 15, 10, 1, 4, 8, 2, 7, 4, 12, 6, 8, 2, 13, 1, 7, 8, 3, 5, 2, 9, 1, 7, 1, 6, 8], [1, 0]]]\n",
      "949\n",
      "[[[43, 22, 4, 15, 4, 77, 4, 15, 13, 75, 29, 10, 6, 5, 2, 3, 5, 8, 9, 3, 14, 10, 11, 8, 2, 3, 13, 15, 1, 1, 2, 6, 5, 19, 20, 6, 2, 9, 8, 13, 1, 12, 6, 4, 10, 6, 5, 2, 1, 7, 1, 8, 2, 8, 30, 77, 4, 15, 13, 75, 8, 2, 4, 7, 2, 15, 1, 1, 2, 6, 5, 19, 20, 6, 2, 9, 2, 9, 1, 25, 6, 12, 2, 6, 15, 8, 3, 21, 6, 10, 10, 1, 19, 4, 10, 6, 15, 15, 6, 19, 7, 4, 2, 6, 3, 5, 17], [1, 0]], [[35, 27, 8, 13, 7, 1, 8, 6, 11, 1, 5, 2, 30, 8, 9, 1, 20, 6, 10, 10, 11, 3, 20, 9, 4, 2, 6, 8, 7, 6, 19, 9, 2, 21, 3, 7, 3, 14, 7, 5, 4, 2, 6, 3, 5, 30, 5, 3, 2, 20, 9, 4, 2, 6, 8, 13, 3, 10, 6, 2, 6, 12, 4, 10, 10, 18, 1, 53, 13, 1, 11, 6, 1, 5, 2, 17, 78, 63, 33, 32, 9, 14, 2, 2, 10, 1, 29, 36, 40, 62, 1, 10, 10, 18, 3, 5, 31, 6, 10, 10, 4, 7, 18], [0, 1]], [[35, 39, 9, 1, 5, 20, 1, 4, 7, 1, 6, 5, 11, 6, 25, 6, 8, 6, 22, 10, 1, 30, 20, 1, 4, 7, 1, 6, 5, 25, 6, 5, 12, 6, 22, 10, 1, 17, 35, 63, 33, 29, 3, 7, 18, 50, 3, 3, 26, 1, 7, 4, 2, 2, 9, 1, 33, 36, 1, 15, 29, 3, 5, 25, 1, 5, 2, 6, 3, 5, 10, 4, 8, 2, 5, 6, 19, 9, 2, 46, 36, 1, 15, 8, 28, 5, 42, 31, 49, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 49, 48, 66, 68, 43, 25, 14, 48, 14, 73], [0, 1]], [[36, 3, 5, 4, 10, 11, 23, 7, 14, 15, 13, 8, 4, 18, 8, 9, 1, 45, 8, 35, 2, 9, 1, 7, 1, 4, 10, 21, 7, 6, 1, 5, 11, 35, 3, 21, 2, 9, 1, 49, 44, 50, 23, 12, 3, 15, 15, 14, 5, 6, 2, 18, 17, 60, 1, 4, 9, 30, 5, 3, 17, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 23, 39, 20, 73, 62, 29, 68, 27, 40, 32], [0, 1]], [[31, 6, 10, 10, 4, 7, 18, 4, 12, 12, 1, 13, 2, 1, 11, 2, 9, 1, 36, 1, 15, 3, 12, 7, 4, 2, 6, 12, 5, 3, 15, 6, 5, 4, 2, 6, 3, 5, 21, 3, 7, 13, 7, 1, 8, 6, 11, 1, 5, 2, 3, 5, 23, 9, 14, 7, 8, 11, 4, 18, 17, 82, 23, 9, 1, 7, 1, 20, 1, 7, 1, 22, 4, 10, 10, 3, 3, 5, 8, 17, 81, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 18, 55, 40, 42, 44, 10, 15, 60, 26, 40, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 51, 22, 9, 48, 56, 69, 43, 64, 20, 53], [0, 1]]]\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "train_all = [[train_x[i], [train_label[i], 1-train_label[i]]] for i in range(0, len(train_x))]\n",
    "# train_all = [[train_x[i], [train_label[i]]] for i in range(0, len(train_x))]\n",
    "\n",
    "r_index = list(range(len(train_all)))\n",
    "random.shuffle(r_index)\n",
    "train = [train_all[i] for i in r_index[:int(len(r_index)*0.8)]]\n",
    "valid = [train_all[i] for i in r_index[int(len(r_index)*0.8):]]\n",
    "\n",
    "print(len(train))\n",
    "print(train[:5])\n",
    "print(len(valid))\n",
    "print(valid[:5])\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.df)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            print(\"SimpleDataIterator epoch : \", self.epochs)\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences :  [[[35, 44, 1, 2, 3, 5, 23, 7, 14, 15, 13, 45, 8, 49, 6, 8, 2, 35, 1, 15, 4, 6, 10, 21, 7, 3, 15, 2, 9, 1, 40, 34, 29, 20, 4, 8, 5, 3, 2, 4, 14, 2, 9, 3, 7, 6, 58, 1, 11, 17, 28, 4, 15, 8, 1, 10, 21, 21, 14, 5, 11, 6, 5, 19, 15, 18, 12, 4, 15, 13, 4, 6, 19, 5, 38, 36, 3, 5, 3, 2, 13, 4, 18, 17, 41, 15, 4, 6, 10, 24, 9, 2, 2, 13, 8, 24, 16, 16, 2, 17, 12, 3, 16, 57, 9, 62, 39, 69, 32, 8, 53, 58, 66], [1, 0]], [[28, 5, 10, 6, 19, 9, 2, 3, 21, 33, 26, 14, 7, 2, 1, 6, 12, 9, 1, 5, 20, 4, 10, 11, 45, 8, 7, 1, 13, 3, 7, 2, 3, 5, 2, 9, 1, 23, 7, 14, 15, 13, 43, 7, 19, 4, 5, 6, 58, 4, 2, 6, 3, 5, 45, 8, 11, 4, 5, 19, 1, 7, 3, 14, 8, 2, 6, 1, 8, 30, 9, 1, 7, 1, 4, 7, 1, 54, 47, 64, 14, 1, 8, 2, 6, 3, 5, 8, 2, 9, 4, 2, 23, 7, 14, 15, 13, 5, 1, 1, 11, 8, 2, 3, 4, 5, 8, 20, 1, 7, 63, 5, 3, 20, 17], [0, 1]], [[29, 7, 3, 3, 26, 1, 11, 31, 6, 10, 10, 4, 7, 18, 29, 10, 6, 5, 2, 3, 5, 8, 4, 18, 8, 2, 9, 4, 2, 8, 9, 1, 19, 3, 2, 15, 3, 7, 1, 13, 7, 6, 15, 4, 7, 18, 25, 3, 2, 1, 8, 2, 9, 4, 5, 36, 3, 5, 4, 10, 11, 23, 7, 14, 15, 13, 17, 50, 14, 2, 28, 9, 4, 11, 48, 69, 13, 1, 3, 13, 10, 1, 2, 3, 22, 1, 4, 2, 63, 8, 9, 1, 9, 4, 11, 3, 5, 1, 38], [1, 0]], [[43, 5, 32, 4, 2, 14, 7, 11, 4, 18, 4, 19, 7, 1, 4, 2, 15, 4, 5, 30, 41, 10, 6, 1, 39, 6, 1, 8, 1, 10, 30, 13, 4, 8, 8, 1, 11, 4, 20, 4, 18, 17, 23, 9, 1, 20, 3, 7, 10, 11, 6, 8, 4, 22, 1, 2, 2, 1, 7, 13, 10, 4, 12, 1, 22, 1, 12, 4, 14, 8, 1, 3, 21, 9, 6, 15, 4, 5, 11, 9, 6, 8, 22, 1, 10, 6, 1, 21, 2, 9, 4, 2, 19, 3, 3, 11, 12, 4, 5, 2, 7, 6, 14, 15, 13, 9, 3, 25, 1, 7, 1, 25, 6, 10, 38], [1, 0]], [[36, 3, 5, 4, 10, 11, 23, 7, 14, 15, 13, 45, 8, 13, 10, 4, 5, 6, 8, 51, 14, 8, 2, 4, 7, 1, 13, 4, 12, 26, 4, 19, 6, 5, 19, 3, 21, 2, 7, 6, 12, 26, 10, 1, 59, 11, 3, 20, 5, 1, 12, 3, 5, 3, 15, 6, 12, 8, 63, 4, 5, 11, 6, 2, 11, 3, 1, 8, 5, 67, 2, 9, 1, 10, 13, 3, 14, 7, 1, 12, 3, 5, 3, 15, 18, 3, 7, 2, 9, 1, 25, 4, 8, 2, 15, 4, 51, 3, 7, 6, 2, 18, 3, 21, 27, 15, 1, 7, 6, 12, 4, 5, 8, 17], [0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(valid)\n",
    "d = data.next_batch(500)\n",
    "print('Input sequences : ', d[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        max_len = 0\n",
    "        for row in res:\n",
    "            if len(row[0]) > max_len:\n",
    "                max_len = len(row[0])\n",
    "        seqlen = np.array([max_len for i in range(len(res))])\n",
    "        ret = []\n",
    "        label = []\n",
    "        for row in res:\n",
    "            ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "            label.append(row[1][0])\n",
    "        x = np.array(ret)\n",
    "        y = np.array(label)\n",
    "\n",
    "        return x, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences\n",
      " [[42  7  6 15  1  7  3 30 14  5 35  2  4 12  3 22  3 20 10 35 17 60  4  9\n",
      "   3  7  4 30 56  3  1 27  7 13  4  6  3 17 34  4 11  4 15 99  8 64 14  1\n",
      "  11  1 12  6  7 17 46 40 34 29  6  5 29 49 41  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]\n",
      " [37  3  7  1  2  9  4  5 69 47 15  6 10 10  6  3  5 27 15  1  7  6 12  4\n",
      "   5  8  4  7  1  5 45  2  7  1 19  6  8  2  1  7  1 11  2  3 25  3  2  1\n",
      "  17 39  1  9  4 25  1  2  3 11  3 22  1  2  2  1  7 38 44  1  2  7  1 19\n",
      "   6  8  2  1  7  1 11 77  4 15 13 75  8 13  7  1  4 11  2  9  1 20  3  7\n",
      "  11 24  9  2  2 13  8 24 16 16  2 17 12  3 16 28  3 22 52 68 42 72 69 51\n",
      "  25]\n",
      " [23  9  4  5 26 18  3 14 34  1 20  2 38  9  2  2 13  8 24 16 16  2 17 12\n",
      "   3 16 27 12 19 47  9 37 25  7 13  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]]\n",
      "(3, 121)\n",
      "[0 0 1]\n",
      "(3,)\n",
      "[121 121 121]\n"
     ]
    }
   ],
   "source": [
    "data = PaddedDataIterator(train)\n",
    "d = data.next_batch(3)\n",
    "print('Input sequences\\n', d[0])\n",
    "print(d[0].shape)\n",
    "print(d[1])\n",
    "print(d[1].shape)\n",
    "print(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n",
      "256\n",
      "6.64453125\n",
      "0\n",
      "256\n",
      "126\n",
      "(256, 126) ,  (256,) ,  (256,)\n",
      "1\n",
      "256\n",
      "129\n",
      "(256, 129) ,  (256,) ,  (256,)\n",
      "2\n",
      "256\n",
      "125\n",
      "(256, 125) ,  (256,) ,  (256,)\n",
      "3\n",
      "256\n",
      "131\n",
      "(256, 131) ,  (256,) ,  (256,)\n",
      "4\n",
      "256\n",
      "126\n",
      "(256, 126) ,  (256,) ,  (256,)\n",
      "5\n",
      "256\n",
      "128\n",
      "(256, 128) ,  (256,) ,  (256,)\n",
      "6\n",
      "256\n",
      "130\n",
      "(256, 130) ,  (256,) ,  (256,)\n",
      "=====\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def align(data):\n",
    "    print(len(data))\n",
    "    max_len = 0\n",
    "    for row in data:\n",
    "        if len(row[0]) > max_len:\n",
    "            max_len = len(row[0])\n",
    "    print(max_len)\n",
    "    ret = []\n",
    "    label = []\n",
    "    for row in data:\n",
    "        ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "        label.append(row[1][0])\n",
    "    x = np.array(ret)\n",
    "    y = np.array(label)\n",
    "    seq_len = np.array([max_len for i in data])\n",
    "    \n",
    "    return x, y, seq_len\n",
    "\n",
    "# def xx():\n",
    "#     max_len = 0\n",
    "#     for row in res:\n",
    "#         if len(row[0]) > max_len:\n",
    "#             max_len = len(row[0])\n",
    "#     seqlen = np.array([max_len for i in range(len(res))])\n",
    "#     ret = []\n",
    "#     label = []\n",
    "#     for row in res:\n",
    "#         ret += [row[0] + [0]*(max_len-len(row[0]))]\n",
    "#         label.append(row[1][0])\n",
    "#     x = np.array(ret)\n",
    "#     y = np.array(label)\n",
    "\n",
    "# test_length = 1701\n",
    "print(test_length)\n",
    "print(batch_size)\n",
    "print(test_length/batch_size)\n",
    "\n",
    "test_list = []\n",
    "test_addlen = test\n",
    "test_addlen.extend(test[0:batch_size])\n",
    "# for i in range(test_length/batch_size+1):\n",
    "for i in range(math.ceil(test_length/batch_size)):\n",
    "    print(i)\n",
    "    x, y, seq_len = align(test[i*batch_size:(i+1)*batch_size])\n",
    "    print(x.shape, \", \", y.shape, \", \", seq_len.shape)\n",
    "#     print(y.shape)\n",
    "#     print(seq_len.shape)\n",
    "    test_list.append([x, y, seq_len])\n",
    "\n",
    "print(\"=====\")\n",
    "print(len(test_list[0]))\n",
    "    \n",
    "\n",
    "# test_align, max_len = align(test)\n",
    "# print(len(test_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "NUM_LAYERS=2\n",
    "\n",
    "def gruCell(state_size):\n",
    "#     lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_hidden, state_is_tuple=True)\n",
    "    gru_cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    return gru_cell\n",
    "    \n",
    "def build_graph(\n",
    "    vocab_size = len(dictionary),\n",
    "    state_size = 64,\n",
    "    batch_size = 256,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "#     keep_prob = tf.constant(1.0)\n",
    "    print(\"====\",x.shape)\n",
    "#     print(\"====\",y.shape)\n",
    "#     print(\"====\",seqlen.shape)\n",
    "\n",
    "    # Embedding layer\n",
    "#     embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "#     rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "#     cell = tf.nn.rnn_cell.BasicLSTMCell(state_size)\n",
    "#     cell = tf.contrib.rnn.MultiRNNCell([gruCell(state_size) for _ in range(NUM_LAYERS)])\n",
    "    init_state = tf.get_variable('gruCell', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "#     rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    \"\"\"\n",
    "    Obtain the last relevant output. The best approach in the future will be to use:\n",
    "\n",
    "        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    which is the Tensorflow equivalent of numpy's rnn_outputs[range(30), seqlen-1, :], but the\n",
    "    gradient for this op has not been implemented as of this writing.\n",
    "\n",
    "    The below solution works, but throws a UserWarning re: the gradient.\n",
    "    \"\"\"\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "#     print(correct.shape)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "#     print(accuracy.shape)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "#         'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 256, num_epochs = 150, iterator = PaddedDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        tv = iterator(valid)\n",
    "#         te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, tv_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "#             print(\"----\",batch[0].shape)\n",
    "#             print(\"----\",batch[1].shape)\n",
    "#             print(\"----\",batch[2].shape)\n",
    "#             dropout_parameter=np.array(0.6)\n",
    "#             print(\"----\",dropout_parameter.shape)\n",
    "#             feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: dropout_parameter}\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                tv_epoch = tv.epochs\n",
    "                while tv.epochs == tv_epoch:\n",
    "                    step += 1\n",
    "                    batch = tv.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                tv_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- tv:\", tv_losses[-1])\n",
    "            \n",
    "        predictions = []\n",
    "        for te in test_list:\n",
    "            feed = {g['x']: te[0], g['y']: te[1], g['seqlen']: te[2]}\n",
    "            preds_, _ = sess.run([g['preds'], g['ts']], feed_dict=feed)\n",
    "            print(len(preds_))\n",
    "            predictions.extend(preds_)\n",
    "\n",
    "    return tr_losses, tv_losses, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== (256, ?)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape (?, 256) must have rank at least 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-b05ae607043b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtr_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtv_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-1ff40ab3a9f2>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(vocab_size, state_size, batch_size, num_classes)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, sequence_length=seqlen,\n\u001b[0;32m---> 42\u001b[0;31m                                                  initial_state=init_state)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Add dropout, as the model otherwise quickly overfits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   inputs_got_shape = tuple(input_.get_shape().with_rank_at_least(3)\n\u001b[0;32m--> 677\u001b[0;31m                            for input_ in flat_input)\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m   \u001b[0mconst_time_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconst_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_got_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   inputs_got_shape = tuple(input_.get_shape().with_rank_at_least(3)\n\u001b[0;32m--> 677\u001b[0;31m                            for input_ in flat_input)\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m   \u001b[0mconst_time_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconst_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_got_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mwith_rank_at_least\u001b[0;34m(self, rank)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \"\"\"\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape %s must have rank at least %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape (?, 256) must have rank at least 3"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, tv_losses, predictions = train_graph(g)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  9.99783456e-01,   2.16587825e-04], dtype=float32), array([  9.99985099e-01,   1.49040734e-05], dtype=float32), array([  4.22566445e-05,   9.99957800e-01], dtype=float32), array([  9.99928355e-01,   7.15992355e-05], dtype=float32), array([  2.82522287e-05,   9.99971747e-01], dtype=float32), array([  9.99980330e-01,   1.97034951e-05], dtype=float32), array([ 0.0166364 ,  0.98336363], dtype=float32), array([  9.99961138e-01,   3.88448861e-05], dtype=float32), array([  9.99980211e-01,   1.98258767e-05], dtype=float32), array([ 0.84958726,  0.15041277], dtype=float32), array([  9.99469578e-01,   5.30452642e-04], dtype=float32), array([  9.99907732e-01,   9.22226100e-05], dtype=float32), array([  9.99968529e-01,   3.14640856e-05], dtype=float32), array([ 0.91769451,  0.08230553], dtype=float32), array([ 0.61804193,  0.3819581 ], dtype=float32), array([ 0.01473562,  0.98526436], dtype=float32), array([  9.99910355e-01,   8.96546262e-05], dtype=float32), array([  1.98048274e-05,   9.99980211e-01], dtype=float32), array([  9.99952316e-01,   4.76435562e-05], dtype=float32), array([  2.63770053e-05,   9.99973655e-01], dtype=float32), array([  2.93494122e-05,   9.99970675e-01], dtype=float32), array([  9.99966145e-01,   3.38797981e-05], dtype=float32), array([ 0.99684036,  0.00315969], dtype=float32), array([ 0.01311749,  0.98688251], dtype=float32), array([  9.99958515e-01,   4.14377719e-05], dtype=float32), array([  9.99966860e-01,   3.31909578e-05], dtype=float32), array([  2.32643361e-05,   9.99976754e-01], dtype=float32), array([  9.99983907e-01,   1.60593754e-05], dtype=float32), array([  9.99923468e-01,   7.64918732e-05], dtype=float32), array([ 0.0011173 ,  0.99888271], dtype=float32), array([  9.99650002e-01,   3.50023416e-04], dtype=float32), array([  7.42093689e-05,   9.99925733e-01], dtype=float32), array([  9.99908566e-01,   9.14327684e-05], dtype=float32), array([ 0.99761581,  0.00238426], dtype=float32), array([ 0.00811667,  0.99188328], dtype=float32), array([ 0.99560189,  0.00439812], dtype=float32), array([  9.99981642e-01,   1.83119919e-05], dtype=float32), array([ 0.92496699,  0.07503301], dtype=float32), array([ 0.99221498,  0.00778507], dtype=float32), array([  8.52195837e-04,   9.99147773e-01], dtype=float32), array([  9.99981642e-01,   1.83396405e-05], dtype=float32), array([  9.99835253e-01,   1.64734636e-04], dtype=float32), array([  9.99987960e-01,   1.20455506e-05], dtype=float32), array([  9.99984860e-01,   1.50913502e-05], dtype=float32), array([  9.99411225e-01,   5.88775729e-04], dtype=float32), array([ 0.99390996,  0.00608998], dtype=float32), array([  9.99896407e-01,   1.03624719e-04], dtype=float32), array([ 0.06200935,  0.93799067], dtype=float32), array([  9.99784887e-01,   2.15149950e-04], dtype=float32), array([  9.99487162e-01,   5.12867176e-04], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# print(predictions[:test_length])\n",
    "print(predictions[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('csvtest4.csv', 'w') as csvfile:\n",
    "    csvfile.write(\",\".join(['id', 'realDonaldTrump', 'HillaryClinton'])+\"\\n\")\n",
    "    data = []\n",
    "    for i in range(test_length):\n",
    "        csvfile.write(\",\".join([str(i), str(predictions[i][1]), str(predictions[i][0])])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
